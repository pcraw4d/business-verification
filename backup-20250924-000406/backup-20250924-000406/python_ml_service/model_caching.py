#!/usr/bin/env python3
"""
Model Caching for Sub-100ms Response Times

This module implements high-performance model caching to achieve sub-100ms
response times for ML model predictions, including:

- In-memory model caching
- Redis-based distributed caching
- Prediction result caching
- Model loading optimization
- Cache invalidation strategies
- Performance monitoring and metrics

Target: Sub-100ms response times for cached predictions
"""

import os
import json
import time
import logging
import pickle
import hashlib
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from datetime import datetime, timedelta
from pathlib import Path
import threading
from collections import OrderedDict
import warnings
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import redis
from transformers import (
    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,
    BertTokenizer, BertForSequenceClassification,
    DistilBertTokenizer, DistilBertForSequenceClassification
)
import joblib
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ModelCache:
    """High-performance model caching system"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Cache configuration
        self.max_cache_size = config.get('max_cache_size', 1000)
        self.cache_ttl = config.get('cache_ttl', 3600)  # 1 hour
        self.model_cache_size = config.get('model_cache_size', 10)
        self.prediction_cache_size = config.get('prediction_cache_size', 10000)
        
        # Initialize caches
        self.model_cache = OrderedDict()
        self.prediction_cache = OrderedDict()
        self.cache_timestamps = {}
        self.cache_access_counts = {}
        
        # Redis connection (optional)
        self.redis_client = None
        if config.get('redis_enabled', False):
            self._init_redis()
        
        # Thread safety
        self.cache_lock = threading.RLock()
        
        # Performance metrics
        self.cache_metrics = {
            "model_cache_hits": 0,
            "model_cache_misses": 0,
            "prediction_cache_hits": 0,
            "prediction_cache_misses": 0,
            "total_requests": 0,
            "cache_hit_rate": 0.0,
            "average_response_time": 0.0
        }
        
        logger.info(f"üöÄ Model Cache initialized")
        logger.info(f"üì± Device: {self.device}")
        logger.info(f"üíæ Model cache size: {self.model_cache_size}")
        logger.info(f"üíæ Prediction cache size: {self.prediction_cache_size}")
        logger.info(f"‚è∞ Cache TTL: {self.cache_ttl} seconds")
    
    def _init_redis(self):
        """Initialize Redis connection for distributed caching"""
        try:
            redis_config = self.config.get('redis_config', {})
            self.redis_client = redis.Redis(
                host=redis_config.get('host', 'localhost'),
                port=redis_config.get('port', 6379),
                db=redis_config.get('db', 0),
                password=redis_config.get('password', None),
                decode_responses=False
            )
            
            # Test connection
            self.redis_client.ping()
            logger.info("‚úÖ Redis connection established")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis connection failed: {e}")
            self.redis_client = None
    
    def cache_model(self, model_id: str, model: nn.Module, tokenizer: Any) -> bool:
        """Cache a model in memory"""
        with self.cache_lock:
            try:
                # Check if model already cached
                if model_id in self.model_cache:
                    logger.info(f"üîÑ Model {model_id} already cached, updating...")
                
                # Store model and tokenizer
                self.model_cache[model_id] = {
                    'model': model,
                    'tokenizer': tokenizer,
                    'cached_at': time.time(),
                    'access_count': 0
                }
                
                # Move to end (most recently used)
                self.model_cache.move_to_end(model_id)
                
                # Enforce cache size limit
                if len(self.model_cache) > self.model_cache_size:
                    # Remove least recently used model
                    oldest_model_id = next(iter(self.model_cache))
                    del self.model_cache[oldest_model_id]
                    logger.info(f"üóëÔ∏è Removed oldest model {oldest_model_id} from cache")
                
                logger.info(f"‚úÖ Model {model_id} cached successfully")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå Failed to cache model {model_id}: {e}")
                return False
    
    def get_cached_model(self, model_id: str) -> Optional[Dict[str, Any]]:
        """Get cached model"""
        with self.cache_lock:
            if model_id in self.model_cache:
                # Update access count and move to end
                self.model_cache[model_id]['access_count'] += 1
                self.model_cache.move_to_end(model_id)
                
                self.cache_metrics["model_cache_hits"] += 1
                logger.debug(f"üéØ Model cache hit for {model_id}")
                return self.model_cache[model_id]
            else:
                self.cache_metrics["model_cache_misses"] += 1
                logger.debug(f"‚ùå Model cache miss for {model_id}")
                return None
    
    def cache_prediction(self, cache_key: str, prediction: Dict[str, Any]) -> bool:
        """Cache prediction result"""
        with self.cache_lock:
            try:
                # Store prediction
                self.prediction_cache[cache_key] = {
                    'prediction': prediction,
                    'cached_at': time.time(),
                    'access_count': 0
                }
                
                # Move to end (most recently used)
                self.prediction_cache.move_to_end(cache_key)
                
                # Store timestamp
                self.cache_timestamps[cache_key] = time.time()
                
                # Enforce cache size limit
                if len(self.prediction_cache) > self.prediction_cache_size:
                    # Remove least recently used prediction
                    oldest_key = next(iter(self.prediction_cache))
                    del self.prediction_cache[oldest_key]
                    if oldest_key in self.cache_timestamps:
                        del self.cache_timestamps[oldest_key]
                    logger.debug(f"üóëÔ∏è Removed oldest prediction from cache")
                
                logger.debug(f"‚úÖ Prediction cached with key: {cache_key[:20]}...")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå Failed to cache prediction: {e}")
                return False
    
    def get_cached_prediction(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Get cached prediction"""
        with self.cache_lock:
            # Check if prediction exists and is not expired
            if cache_key in self.prediction_cache:
                cached_item = self.prediction_cache[cache_key]
                cached_time = cached_item['cached_at']
                
                # Check TTL
                if time.time() - cached_time < self.cache_ttl:
                    # Update access count and move to end
                    cached_item['access_count'] += 1
                    self.prediction_cache.move_to_end(cache_key)
                    
                    self.cache_metrics["prediction_cache_hits"] += 1
                    logger.debug(f"üéØ Prediction cache hit for key: {cache_key[:20]}...")
                    return cached_item['prediction']
                else:
                    # Expired, remove from cache
                    del self.prediction_cache[cache_key]
                    if cache_key in self.cache_timestamps:
                        del self.cache_timestamps[cache_key]
                    logger.debug(f"‚è∞ Expired prediction removed from cache")
            
            self.cache_metrics["prediction_cache_misses"] += 1
            logger.debug(f"‚ùå Prediction cache miss for key: {cache_key[:20]}...")
            return None
    
    def generate_cache_key(self, text: str, model_id: str, 
                          model_version: str = "1.0.0") -> str:
        """Generate cache key for prediction"""
        # Create hash of input parameters
        key_data = f"{text}_{model_id}_{model_version}"
        cache_key = hashlib.md5(key_data.encode()).hexdigest()
        return cache_key
    
    def predict_with_cache(self, text: str, model_id: str, 
                          model_version: str = "1.0.0",
                          prediction_func: Callable = None) -> Dict[str, Any]:
        """Make prediction with caching"""
        start_time = time.time()
        self.cache_metrics["total_requests"] += 1
        
        # Generate cache key
        cache_key = self.generate_cache_key(text, model_id, model_version)
        
        # Try to get cached prediction
        cached_prediction = self.get_cached_prediction(cache_key)
        if cached_prediction is not None:
            # Add cache hit indicator
            cached_prediction["cache_hit"] = True
            cached_prediction["response_time"] = time.time() - start_time
            return cached_prediction
        
        # Cache miss, make new prediction
        if prediction_func is None:
            raise ValueError("prediction_func is required for cache miss")
        
        # Get cached model
        cached_model = self.get_cached_model(model_id)
        if cached_model is None:
            raise ValueError(f"Model {model_id} not found in cache")
        
        # Make prediction
        prediction = prediction_func(text, cached_model['model'], cached_model['tokenizer'])
        
        # Add cache miss indicator
        prediction["cache_hit"] = False
        prediction["response_time"] = time.time() - start_time
        
        # Cache the prediction
        self.cache_prediction(cache_key, prediction)
        
        return prediction
    
    def preload_models(self, models_config: List[Dict[str, Any]]) -> Dict[str, bool]:
        """Preload models into cache"""
        logger.info(f"üîÑ Preloading {len(models_config)} models...")
        
        preload_results = {}
        
        for model_config in tqdm(models_config, desc="Preloading models"):
            model_id = model_config['model_id']
            model_path = model_config['model_path']
            model_type = model_config.get('model_type', 'bert')
            
            try:
                # Load model and tokenizer
                model, tokenizer = self._load_model_and_tokenizer(model_path, model_type)
                
                # Cache model
                success = self.cache_model(model_id, model, tokenizer)
                preload_results[model_id] = success
                
                if success:
                    logger.info(f"‚úÖ Model {model_id} preloaded successfully")
                else:
                    logger.error(f"‚ùå Failed to preload model {model_id}")
                    
            except Exception as e:
                logger.error(f"‚ùå Error preloading model {model_id}: {e}")
                preload_results[model_id] = False
        
        successful_preloads = sum(preload_results.values())
        logger.info(f"üìä Preloading completed: {successful_preloads}/{len(models_config)} models")
        
        return preload_results
    
    def _load_model_and_tokenizer(self, model_path: str, model_type: str) -> Tuple[nn.Module, Any]:
        """Load model and tokenizer from path"""
        if model_type.lower() == "distilbert":
            model = DistilBertForSequenceClassification.from_pretrained(model_path)
            tokenizer = DistilBertTokenizer.from_pretrained(model_path)
        else:
            model = BertForSequenceClassification.from_pretrained(model_path)
            tokenizer = BertTokenizer.from_pretrained(model_path)
        
        model.to(self.device)
        model.eval()
        
        return model, tokenizer
    
    def warm_up_cache(self, warm_up_data: List[str], model_id: str) -> Dict[str, Any]:
        """Warm up cache with sample predictions"""
        logger.info(f"üî• Warming up cache for model {model_id} with {len(warm_up_data)} samples...")
        
        warm_up_results = {
            "model_id": model_id,
            "samples_processed": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "average_response_time": 0.0,
            "warm_up_time": 0.0
        }
        
        start_time = time.time()
        response_times = []
        
        # Get cached model
        cached_model = self.get_cached_model(model_id)
        if cached_model is None:
            logger.error(f"‚ùå Model {model_id} not found in cache for warm-up")
            return warm_up_results
        
        def prediction_func(text, model, tokenizer):
            # Simple prediction function for warm-up
            inputs = tokenizer(
                text,
                max_length=512,
                padding=True,
                truncation=True,
                return_tensors="pt"
            )
            
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                prediction = torch.argmax(logits, dim=-1).item()
                confidence = torch.max(torch.softmax(logits, dim=-1), dim=-1)[0].item()
            
            return {
                "prediction": prediction,
                "confidence": confidence,
                "model_id": model_id
            }
        
        # Process warm-up data
        for text in tqdm(warm_up_data, desc="Warming up cache"):
            try:
                result = self.predict_with_cache(text, model_id, prediction_func=prediction_func)
                warm_up_results["samples_processed"] += 1
                
                if result["cache_hit"]:
                    warm_up_results["cache_hits"] += 1
                else:
                    warm_up_results["cache_misses"] += 1
                
                response_times.append(result["response_time"])
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Warm-up failed for text: {e}")
        
        # Calculate metrics
        warm_up_results["warm_up_time"] = time.time() - start_time
        warm_up_results["average_response_time"] = np.mean(response_times) if response_times else 0.0
        
        logger.info(f"üî• Cache warm-up completed:")
        logger.info(f"   Samples processed: {warm_up_results['samples_processed']}")
        logger.info(f"   Cache hits: {warm_up_results['cache_hits']}")
        logger.info(f"   Cache misses: {warm_up_results['cache_misses']}")
        logger.info(f"   Average response time: {warm_up_results['average_response_time']*1000:.2f}ms")
        
        return warm_up_results
    
    def clear_cache(self, cache_type: str = "all") -> bool:
        """Clear cache"""
        with self.cache_lock:
            try:
                if cache_type in ["all", "models"]:
                    self.model_cache.clear()
                    logger.info("üóëÔ∏è Model cache cleared")
                
                if cache_type in ["all", "predictions"]:
                    self.prediction_cache.clear()
                    self.cache_timestamps.clear()
                    logger.info("üóëÔ∏è Prediction cache cleared")
                
                return True
                
            except Exception as e:
                logger.error(f"‚ùå Failed to clear cache: {e}")
                return False
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        with self.cache_lock:
            # Calculate cache hit rate
            total_hits = self.cache_metrics["model_cache_hits"] + self.cache_metrics["prediction_cache_hits"]
            total_misses = self.cache_metrics["model_cache_misses"] + self.cache_metrics["prediction_cache_misses"]
            total_requests = total_hits + total_misses
            
            cache_hit_rate = total_hits / total_requests if total_requests > 0 else 0.0
            
            # Calculate average response time
            if self.cache_metrics["total_requests"] > 0:
                avg_response_time = self.cache_metrics["average_response_time"]
            else:
                avg_response_time = 0.0
            
            cache_stats = {
                "model_cache": {
                    "size": len(self.model_cache),
                    "max_size": self.model_cache_size,
                    "hits": self.cache_metrics["model_cache_hits"],
                    "misses": self.cache_metrics["model_cache_misses"],
                    "hit_rate": self.cache_metrics["model_cache_hits"] / 
                               (self.cache_metrics["model_cache_hits"] + self.cache_metrics["model_cache_misses"]) 
                               if (self.cache_metrics["model_cache_hits"] + self.cache_metrics["model_cache_misses"]) > 0 else 0.0
                },
                "prediction_cache": {
                    "size": len(self.prediction_cache),
                    "max_size": self.prediction_cache_size,
                    "hits": self.cache_metrics["prediction_cache_hits"],
                    "misses": self.cache_metrics["prediction_cache_misses"],
                    "hit_rate": self.cache_metrics["prediction_cache_hits"] / 
                               (self.cache_metrics["prediction_cache_hits"] + self.cache_metrics["prediction_cache_misses"]) 
                               if (self.cache_metrics["prediction_cache_hits"] + self.cache_metrics["prediction_cache_misses"]) > 0 else 0.0
                },
                "overall": {
                    "total_requests": self.cache_metrics["total_requests"],
                    "cache_hit_rate": cache_hit_rate,
                    "average_response_time": avg_response_time,
                    "cache_ttl": self.cache_ttl
                },
                "timestamp": datetime.now().isoformat()
            }
            
            return cache_stats
    
    def benchmark_cache_performance(self, test_data: List[str], model_id: str,
                                  num_runs: int = 100) -> Dict[str, Any]:
        """Benchmark cache performance"""
        logger.info(f"‚è±Ô∏è Benchmarking cache performance with {num_runs} runs...")
        
        benchmark_results = {
            "model_id": model_id,
            "test_samples": len(test_data),
            "num_runs": num_runs,
            "cache_performance": {},
            "response_times": {},
            "timestamp": datetime.now().isoformat()
        }
        
        # Test with cache
        cache_times = []
        cache_hits = 0
        
        for run in range(num_runs):
            for text in test_data:
                start_time = time.time()
                
                # Make prediction with cache
                result = self.predict_with_cache(text, model_id)
                
                response_time = time.time() - start_time
                cache_times.append(response_time)
                
                if result["cache_hit"]:
                    cache_hits += 1
        
        # Test without cache (clear cache first)
        self.clear_cache("predictions")
        no_cache_times = []
        
        for run in range(num_runs):
            for text in test_data:
                start_time = time.time()
                
                # Make prediction without cache
                result = self.predict_with_cache(text, model_id)
                
                response_time = time.time() - start_time
                no_cache_times.append(response_time)
        
        # Calculate performance metrics
        cache_avg_time = np.mean(cache_times)
        no_cache_avg_time = np.mean(no_cache_times)
        speedup = no_cache_avg_time / cache_avg_time if cache_avg_time > 0 else 0
        
        benchmark_results["cache_performance"] = {
            "cache_hit_rate": cache_hits / (num_runs * len(test_data)),
            "speedup_factor": speedup,
            "cache_avg_time_ms": cache_avg_time * 1000,
            "no_cache_avg_time_ms": no_cache_avg_time * 1000
        }
        
        benchmark_results["response_times"] = {
            "cache_times": cache_times,
            "no_cache_times": no_cache_times,
            "cache_p95": np.percentile(cache_times, 95) * 1000,
            "cache_p99": np.percentile(cache_times, 99) * 1000,
            "no_cache_p95": np.percentile(no_cache_times, 95) * 1000,
            "no_cache_p99": np.percentile(no_cache_times, 99) * 1000
        }
        
        logger.info(f"üìä Cache performance benchmark results:")
        logger.info(f"   Cache hit rate: {benchmark_results['cache_performance']['cache_hit_rate']:.4f}")
        logger.info(f"   Speedup factor: {speedup:.2f}x")
        logger.info(f"   Cache avg time: {cache_avg_time*1000:.2f}ms")
        logger.info(f"   No cache avg time: {no_cache_avg_time*1000:.2f}ms")
        
        return benchmark_results
    
    def create_cache_report(self, benchmark_results: Dict[str, Any]) -> str:
        """Create comprehensive cache performance report"""
        logger.info("üìù Creating cache performance report...")
        
        cache_stats = self.get_cache_stats()
        
        report = {
            "cache_statistics": cache_stats,
            "benchmark_results": benchmark_results,
            "recommendations": self._generate_cache_recommendations(cache_stats, benchmark_results),
            "timestamp": datetime.now().isoformat()
        }
        
        # Save report
        report_path = Path("cache_reports") / f"cache_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        # Create visualization
        self._create_cache_visualizations(cache_stats, benchmark_results)
        
        logger.info(f"üìù Cache report saved to: {report_path}")
        return str(report_path)
    
    def _generate_cache_recommendations(self, cache_stats: Dict[str, Any], 
                                      benchmark_results: Dict[str, Any]) -> List[str]:
        """Generate cache optimization recommendations"""
        recommendations = []
        
        # Analyze cache hit rate
        overall_hit_rate = cache_stats["overall"]["cache_hit_rate"]
        if overall_hit_rate >= 0.8:
            recommendations.append("‚úÖ Excellent cache hit rate! Cache is performing optimally.")
        elif overall_hit_rate >= 0.6:
            recommendations.append("‚úÖ Good cache hit rate. Consider increasing cache size for better performance.")
        else:
            recommendations.append("‚ö†Ô∏è Low cache hit rate. Review cache strategy and TTL settings.")
        
        # Analyze response times
        avg_response_time = cache_stats["overall"]["average_response_time"]
        if avg_response_time <= 0.1:  # 100ms
            recommendations.append("‚úÖ Response times meet sub-100ms target!")
        elif avg_response_time <= 0.2:  # 200ms
            recommendations.append("‚úÖ Good response times. Consider further optimization.")
        else:
            recommendations.append("‚ö†Ô∏è Response times above target. Review model optimization and caching strategy.")
        
        # Analyze speedup
        if "cache_performance" in benchmark_results:
            speedup = benchmark_results["cache_performance"]["speedup_factor"]
            if speedup >= 5.0:
                recommendations.append("‚úÖ Excellent cache speedup! Cache provides significant performance benefits.")
            elif speedup >= 2.0:
                recommendations.append("‚úÖ Good cache speedup. Cache is providing meaningful performance improvements.")
            else:
                recommendations.append("‚ö†Ô∏è Limited cache speedup. Review cache implementation and strategy.")
        
        recommendations.extend([
            "üí° Consider Redis for distributed caching in production.",
            "üí° Implement cache warming for frequently used models.",
            "üí° Monitor cache hit rates and adjust TTL accordingly.",
            "üí° Use cache compression for memory optimization.",
            "üí° Implement cache eviction policies based on access patterns."
        ])
        
        return recommendations
    
    def _create_cache_visualizations(self, cache_stats: Dict[str, Any], 
                                   benchmark_results: Dict[str, Any]):
        """Create cache performance visualizations"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # Cache hit rates
            cache_types = ["Model Cache", "Prediction Cache"]
            hit_rates = [
                cache_stats["model_cache"]["hit_rate"],
                cache_stats["prediction_cache"]["hit_rate"]
            ]
            
            axes[0, 0].bar(cache_types, hit_rates, color=['skyblue', 'lightgreen'])
            axes[0, 0].set_title('Cache Hit Rates')
            axes[0, 0].set_ylabel('Hit Rate')
            axes[0, 0].set_ylim(0, 1)
            
            # Cache sizes
            cache_sizes = [
                cache_stats["model_cache"]["size"],
                cache_stats["prediction_cache"]["size"]
            ]
            max_sizes = [
                cache_stats["model_cache"]["max_size"],
                cache_stats["prediction_cache"]["max_size"]
            ]
            
            x = np.arange(len(cache_types))
            width = 0.35
            
            axes[0, 1].bar(x - width/2, cache_sizes, width, label='Current Size', color='lightcoral')
            axes[0, 1].bar(x + width/2, max_sizes, width, label='Max Size', color='lightblue')
            axes[0, 1].set_title('Cache Sizes')
            axes[0, 1].set_ylabel('Size')
            axes[0, 1].set_xticks(x)
            axes[0, 1].set_xticklabels(cache_types)
            axes[0, 1].legend()
            
            # Response time comparison
            if "response_times" in benchmark_results:
                cache_times = benchmark_results["response_times"]["cache_times"]
                no_cache_times = benchmark_results["response_times"]["no_cache_times"]
                
                axes[1, 0].hist(cache_times, bins=20, alpha=0.7, label='With Cache', color='lightgreen')
                axes[1, 0].hist(no_cache_times, bins=20, alpha=0.7, label='Without Cache', color='lightcoral')
                axes[1, 0].set_title('Response Time Distribution')
                axes[1, 0].set_xlabel('Response Time (seconds)')
                axes[1, 0].set_ylabel('Frequency')
                axes[1, 0].legend()
            
            # Performance metrics
            if "cache_performance" in benchmark_results:
                metrics = ["Cache Hit Rate", "Speedup Factor"]
                values = [
                    benchmark_results["cache_performance"]["cache_hit_rate"],
                    benchmark_results["cache_performance"]["speedup_factor"]
                ]
                
                axes[1, 1].bar(metrics, values, color=['orange', 'purple'])
                axes[1, 1].set_title('Cache Performance Metrics')
                axes[1, 1].set_ylabel('Value')
            
            plt.tight_layout()
            
            # Save visualization
            viz_path = Path("cache_reports") / f"cache_visualization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            viz_path.parent.mkdir(exist_ok=True)
            plt.savefig(viz_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("üìä Cache visualizations created")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to create cache visualizations: {e}")

def main():
    """Main function to demonstrate model caching"""
    
    # Configuration
    config = {
        'max_cache_size': 1000,
        'cache_ttl': 3600,
        'model_cache_size': 10,
        'prediction_cache_size': 10000,
        'redis_enabled': False
    }
    
    # Initialize model cache
    cache = ModelCache(config)
    
    # Example usage
    test_texts = [
        "Acme Corporation - Leading technology solutions provider",
        "Healthcare Plus - Comprehensive medical services",
        "Financial Services Inc - Investment and wealth management",
        "Retail Solutions - Fashion and lifestyle products"
    ]
    
    # Create dummy model for demonstration
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(10, 4)
        
        def forward(self, x):
            return self.linear(x)
    
    dummy_model = DummyModel()
    dummy_tokenizer = None  # Would be actual tokenizer in real implementation
    
    # Cache model
    cache.cache_model("dummy_model", dummy_model, dummy_tokenizer)
    
    # Get cache stats
    cache_stats = cache.get_cache_stats()
    print(f"Cache stats: {cache_stats}")
    
    # Benchmark cache performance
    benchmark_results = cache.benchmark_cache_performance(test_texts, "dummy_model")
    print(f"Benchmark results: {benchmark_results}")
    
    # Create cache report
    report_path = cache.create_cache_report(benchmark_results)
    print(f"Cache report: {report_path}")
    
    logger.info("üéâ Model caching demonstration completed!")

if __name__ == "__main__":
    main()
