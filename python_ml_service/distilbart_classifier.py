#!/usr/bin/env python3
"""
DistilBART Model for Business Classification, Summarization, and Explanation

This module implements DistilBART (Distilled BART) for multi-task business classification:
- Zero-shot classification (replaces BERT)
- Content summarization
- Explanation generation
- Model quantization for optimization

Model: sshleifer/distilbart-cnn-12-6 (for summarization)
Model: typeform/distilbert-base-uncased-mnli (for zero-shot classification)
"""

import os
import json
import time
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from pathlib import Path

import torch
import torch.nn as nn
from torch.quantization import quantize_dynamic
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline
)
import numpy as np

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DistilBARTBusinessClassifier:
    """DistilBART Business Classifier with Multi-Task Capabilities and Quantization"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Model paths
        self.model_save_path = Path(config.get('model_save_path', 'models/distilbart'))
        self.model_save_path.mkdir(parents=True, exist_ok=True)
        
        self.quantized_models_path = Path(config.get('quantized_models_path', 'models/quantized'))
        self.quantized_models_path.mkdir(parents=True, exist_ok=True)
        
        # Quantization settings
        self.use_quantization = config.get('use_quantization', True)
        self.quantization_dtype = config.get('quantization_dtype', torch.qint8)
        
        # Industry labels for zero-shot classification
        self.industry_labels = config.get('industry_labels', [
            "Technology", "Healthcare", "Financial Services",
            "Retail", "Food & Beverage", "Manufacturing",
            "Construction", "Real Estate", "Transportation",
            "Education", "Professional Services", "Agriculture",
            "Mining & Energy", "Utilities", "Wholesale Trade",
            "Arts & Entertainment", "Accommodation & Hospitality",
            "Administrative Services", "Other Services"
        ])
        
        # Initialize models
        self.classifier = None
        self.summarizer = None
        self.tokenizer = None
        self.quantized_classifier = None
        self.quantized_summarizer = None
        
        logger.info(f"ðŸš€ DistilBART Business Classifier initializing on {self.device}")
        self._load_models()
        
        # Quantize models if enabled
        if self.use_quantization:
            self._quantize_models()
    
    def _load_models(self):
        """Load DistilBART models for classification and summarization"""
        try:
            # Set Hugging Face cache directory to writable location
            cache_dir = os.getenv('TRANSFORMERS_CACHE', os.getenv('HF_HOME', '/app/.cache/huggingface'))
            os.makedirs(cache_dir, exist_ok=True)
            logger.info(f"ðŸ“ Using Hugging Face cache directory: {cache_dir}")
            
            # Load zero-shot classification model (DistilBERT-MNLI - already optimized)
            logger.info("ðŸ“¥ Loading DistilBERT-MNLI for classification...")
            self.classifier = pipeline(
                "zero-shot-classification",
                model="typeform/distilbert-base-uncased-mnli",
                device=0 if torch.cuda.is_available() else -1,
                cache_dir=cache_dir
            )
            logger.info("âœ… DistilBERT classification model loaded")
            
            # Load summarization model (DistilBART - 6x smaller than BART-large)
            logger.info("ðŸ“¥ Loading DistilBART for summarization...")
            self.summarizer = pipeline(
                "summarization",
                model="sshleifer/distilbart-cnn-12-6",
                device=0 if torch.cuda.is_available() else -1,
                cache_dir=cache_dir
            )
            logger.info("âœ… DistilBART summarization model loaded")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load DistilBART models: {e}")
            raise
    
    def _quantize_models(self):
        """Quantize models for faster inference and reduced memory"""
        try:
            logger.info("âš¡ Quantizing models for optimization...")
            
            # Note: Pipeline models are wrapped, so we need to access the underlying model
            # For zero-shot classification, the model is already optimized (DistilBERT)
            # We'll focus on quantizing the summarization model if possible
            
            # Try to quantize summarization model
            if hasattr(self.summarizer, 'model') and hasattr(self.summarizer.model, 'model'):
                try:
                    logger.info("âš¡ Quantizing summarization model...")
                    # Access the underlying transformer model
                    base_model = self.summarizer.model.model
                    if base_model is not None:
                        # Set to eval mode for quantization
                        base_model.eval()
                        
                        # Quantize the model
                        quantized_base_model = quantize_dynamic(
                            base_model,
                            {nn.Linear},
                            dtype=self.quantization_dtype
                        )
                        
                        # Replace the model in the pipeline
                        self.summarizer.model.model = quantized_base_model
                        self.quantized_summarizer = self.summarizer
                        logger.info("âœ… Summarization model quantized")
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not quantize summarization model: {e}")
                    logger.info("â„¹ï¸ Using original summarization model")
            
            # Save quantized models metadata
            self._save_quantized_models()
            
        except Exception as e:
            logger.warning(f"âš ï¸ Quantization failed, using original models: {e}")
            self.use_quantization = False
    
    def _save_quantized_models(self):
        """Save quantized models metadata for future use"""
        try:
            quantized_path = self.quantized_models_path / "distilbart"
            quantized_path.mkdir(parents=True, exist_ok=True)
            
            # Save model metadata
            metadata = {
                "quantization_dtype": str(self.quantization_dtype),
                "quantized_at": datetime.now().isoformat(),
                "model_type": "distilbart",
                "quantization_method": "dynamic_int8",
                "classification_model": "typeform/distilbert-base-uncased-mnli",
                "summarization_model": "sshleifer/distilbart-cnn-12-6",
                "quantization_enabled": self.use_quantization
            }
            
            with open(quantized_path / "metadata.json", "w") as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"ðŸ’¾ Quantized models metadata saved to: {quantized_path}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save quantized models metadata: {e}")
    
    def _validate_content_quality(self, content: str, business_name: str) -> Dict[str, Any]:
        """
        Validate content quality and provide recommendations
        
        Returns:
            Dictionary with quality assessment and recommendations
        """
        MIN_CONTENT_FOR_CLASSIFICATION = 20  # Absolute minimum (business name)
        RECOMMENDED_CONTENT_LENGTH = 100     # Recommended for good accuracy
        OPTIMAL_CONTENT_LENGTH = 500        # Optimal for best results
        
        combined = self._combine_content(content, business_name)
        length = len(combined)
        
        quality = "optimal"
        recommendation = ""
        if length < MIN_CONTENT_FOR_CLASSIFICATION:
            quality = "insufficient"
            recommendation = "Content is too short. Please provide business description or website content."
        elif length < RECOMMENDED_CONTENT_LENGTH:
            quality = "minimal"
            recommendation = "Content is minimal. Classification accuracy may be reduced. Consider providing more details."
        elif length < OPTIMAL_CONTENT_LENGTH:
            quality = "good"
            recommendation = "Content is sufficient for classification."
        else:
            quality = "optimal"
            recommendation = "Content is optimal for best classification accuracy."
        
        return {
            "quality": quality,
            "length": length,
            "recommendation": recommendation,
            "min_threshold": MIN_CONTENT_FOR_CLASSIFICATION,
            "recommended_threshold": RECOMMENDED_CONTENT_LENGTH,
            "optimal_threshold": OPTIMAL_CONTENT_LENGTH
        }
    
    def _combine_content(self, content: str, business_name: str, description: str = "") -> str:
        """
        Intelligently combine all available content sources
        
        Args:
            content: Website content or primary content
            business_name: Business name
            description: Business description (optional)
            
        Returns:
            Combined content string
        """
        parts = []
        
        # 1. Business name (always include first)
        if business_name and business_name.strip():
            parts.append(f"Business: {business_name}")
        
        # 2. Description (if provided and meaningful)
        if description and len(description.strip()) > 10:
            parts.append(f"Description: {description}")
        
        # 3. Website content (if provided and meaningful)
        if content and len(content.strip()) > 20:
            # Remove business name if already in content to avoid duplication
            content_lower = content.lower()
            if business_name and business_name.lower() in content_lower:
                # Business name is already in content, use content as-is
                parts.append(f"Website: {content}")
            else:
                parts.append(f"Website: {content}")
        
        combined = ". ".join(parts)
        
        # Ensure minimum length by including business name if combined is too short
        if len(combined) < 20 and business_name:
            combined = f"{business_name}. {combined}".strip()
        
        return combined.strip()
    
    def classify_with_enhancement(
        self,
        content: str,
        business_name: str,
        max_length: int = 1024,
        description: str = ""
    ) -> Dict[str, Any]:
        """
        Classify business with full enhancement (classification + summarization + explanation)
        Uses quantized models if available for faster inference
        
        Args:
            content: Website content or business description
            business_name: Business name
            max_length: Maximum content length for processing
            description: Business description (optional, for better context)
            
        Returns:
            Dictionary with classification, summarization, and explanation
        """
        start_time = time.time()
        
        # Step 1: Combine all available content sources intelligently
        combined_content = self._combine_content(content, business_name, description)
        
        # Step 2: Validate content quality
        quality_assessment = self._validate_content_quality(content, business_name)
        logger.info(f"ðŸ“Š Content quality: {quality_assessment['quality']} ({quality_assessment['length']} chars)")
        
        if quality_assessment['quality'] == "insufficient":
            logger.warning(f"âš ï¸ {quality_assessment['recommendation']}")
        elif quality_assessment['quality'] == "minimal":
            logger.warning(f"âš ï¸ {quality_assessment['recommendation']}")
        else:
            logger.info(f"âœ… {quality_assessment['recommendation']}")
        
        # Step 3: Truncate content if needed (after combination)
        if len(combined_content) > max_length:
            logger.info(f"ðŸ“ Combined content truncated to {max_length} characters (from {len(combined_content)})")
            combined_content = combined_content[:max_length]
        
        # Use quantized models if available
        classifier = self.classifier
        summarizer = self.quantized_summarizer if self.use_quantization and self.quantized_summarizer else self.summarizer
        
        # Step 4: Zero-shot classification
        logger.info(f"ðŸ” Classifying business: {business_name}")
        
        # Validate inputs
        if not combined_content or not combined_content.strip():
            raise ValueError(f"Content cannot be empty for classification. Original content length: {len(content) if content else 0}, business_name: {business_name}")
        
        if not self.industry_labels:
            raise ValueError("Industry labels cannot be empty")
        
        # Log content info for debugging
        logger.info(f"ðŸ“Š Classification input - Content length: {len(combined_content)}, Labels: {len(self.industry_labels)}")
        logger.debug(f"ðŸ“ Content preview: {combined_content[:200]}...")
        
        try:
            classification_result = classifier(
                combined_content,
                self.industry_labels,
                multi_label=False
            )
        except Exception as e:
            logger.error(f"âŒ Classification failed: {e}")
            logger.error(f"   Combined content length: {len(combined_content) if combined_content else 0}")
            logger.error(f"   Original content length: {len(content) if content else 0}")
            logger.error(f"   Business name: {business_name}")
            logger.error(f"   Description length: {len(description) if description else 0}")
            logger.error(f"   Industry labels count: {len(self.industry_labels)}")
            logger.error(f"   Content quality: {quality_assessment['quality']}")
            logger.error(f"   Content preview: {combined_content[:500] if combined_content else 'EMPTY'}")
            raise
        
        # Step 5: Summarize content (use combined_content for consistency)
        logger.info("ðŸ“ Summarizing website content...")
        summary = ""
        try:
            # Only summarize if we have enough content (50+ chars)
            if combined_content and len(combined_content.strip()) > 50:
                summary_result = summarizer(
                    combined_content,
                    max_length=150,
                    min_length=50,
                    do_sample=False
                )
                summary = summary_result[0]['summary_text']
                logger.info(f"âœ… Summary generated ({len(summary)} chars)")
            else:
                logger.info("â„¹ï¸ Content too short for summarization, skipping (minimum 50 chars required)")
        except Exception as e:
            logger.warning(f"âš ï¸ Summarization failed: {e}")
            summary = ""
        
        # Step 6: Generate explanation
        logger.info("ðŸ’¡ Generating classification explanation...")
        explanation = self._generate_explanation(
            business_name,
            classification_result['labels'][0],
            classification_result['scores'][0],
            summary,
            classification_result,
            content=combined_content,  # Pass combined content for better explanation
            quality_assessment=quality_assessment  # Include quality info
        )
        
        processing_time = time.time() - start_time
        
        result = {
            'industry': classification_result['labels'][0],
            'confidence': classification_result['scores'][0],
            'all_scores': dict(zip(
                classification_result['labels'],
                classification_result['scores']
            )),
            'summary': summary,
            'explanation': explanation,
            'processing_time': processing_time,
            'model': 'distilbart-quantized' if self.use_quantization else 'distilbart',
            'quantization_enabled': self.use_quantization,
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… Classification completed in {processing_time:.2f}s (quantized: {self.use_quantization})")
        return result
    
    def classify_only(
        self,
        content: str,
        max_length: int = 1024
    ) -> Dict[str, Any]:
        """
        Classification only (no summarization) - for fast paths
        Uses quantized model if available
        
        Args:
            content: Website content or business description
            max_length: Maximum content length
            
        Returns:
            Classification result only
        """
        if len(content) > max_length:
            content = content[:max_length]
        
        # Use classifier (DistilBERT-MNLI pipeline is already optimized)
        classifier = self.classifier
        
        try:
            classification_result = classifier(
                content,
                self.industry_labels,
                multi_label=False
            )
        except Exception as e:
            logger.error(f"âŒ Classification failed: {e}")
            raise
        
        return {
            'industry': classification_result['labels'][0],
            'confidence': classification_result['scores'][0],
            'all_scores': dict(zip(
                classification_result['labels'],
                classification_result['scores']
            )),
            'model': 'distilbart-quantized' if self.use_quantization else 'distilbart',
            'quantization_enabled': self.use_quantization
        }
    
    def _generate_explanation(
        self,
        business_name: str,
        industry: str,
        confidence: float,
        summary: str,
        classification_result: Dict[str, Any],
        content: str = "",
        quality_assessment: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Generate human-readable explanation for classification
        
        Args:
            business_name: Name of the business
            industry: Detected industry
            confidence: Confidence score
            summary: Content summary
            classification_result: Full classification result
            
        Returns:
            Human-readable explanation string
        """
        # Extract top 3 indicators from classification scores
        top_industries = sorted(
            zip(classification_result['labels'], classification_result['scores']),
            key=lambda x: x[1],
            reverse=True
        )[:3]
        
        # Build explanation
        explanation_parts = [
            f"{business_name} has been classified as **{industry}** "
            f"with {confidence:.1%} confidence."
        ]
        
        # Add confidence context
        if confidence >= 0.9:
            explanation_parts.append("This is a high-confidence classification.")
        elif confidence >= 0.7:
            explanation_parts.append("This is a moderate-confidence classification.")
        else:
            explanation_parts.append("This classification has lower confidence and may require review.")
        
        # Add content quality context if available
        if quality_assessment:
            quality = quality_assessment.get('quality', 'unknown')
            if quality == "insufficient" or quality == "minimal":
                explanation_parts.append(
                    f"Note: Classification was based on limited content ({quality_assessment.get('length', 0)} characters). "
                    "More detailed information may improve accuracy."
                )
        
        # Add alternative industries if significant
        if len(top_industries) > 1 and top_industries[1][1] > 0.3:
            alternatives = [ind[0] for ind in top_industries[1:3] if ind[1] > 0.3]
            if alternatives:
                explanation_parts.append(
                    f"Alternative classifications considered: {', '.join(alternatives)}."
                )
        
        # Add summary context
        if summary:
            explanation_parts.append(
                f"Website content analysis indicates: {summary[:200]}"
            )
        
        return " ".join(explanation_parts)
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about loaded models"""
        return {
            'classification_model': 'typeform/distilbert-base-uncased-mnli',
            'summarization_model': 'sshleifer/distilbart-cnn-12-6',
            'quantization_enabled': self.use_quantization,
            'quantization_dtype': str(self.quantization_dtype),
            'device': str(self.device),
            'model_size_original': '~810MB',
            'model_size_quantized': '~202MB' if self.use_quantization else None,
            'size_reduction': '75%' if self.use_quantization else None,
            'industry_labels_count': len(self.industry_labels)
        }

