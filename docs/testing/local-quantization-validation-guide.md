# Local Quantization Validation Guide

**Purpose**: Validate DistilBART quantization locally before production deployment  
**Estimated Time**: 30-45 minutes  
**Prerequisites**: Docker, Python 3.10+, Go 1.22+

---

## Overview

This guide walks you through validating the quantized DistilBART models locally to ensure:
- ‚úÖ Quantization is working correctly
- ‚úÖ Performance improvements meet expectations (36% faster, 75% smaller)
- ‚úÖ Accuracy is maintained (>85% confidence)
- ‚úÖ All classification features work as expected
- ‚úÖ No regressions in functionality

---

## Prerequisites

### 1. System Requirements

- **Docker** 20.10+ and **Docker Compose** 2.0+
- **Python** 3.10+ (for running benchmarks)
- **Go** 1.22+ (for running integration tests)
- **Memory**: 4GB+ RAM available
- **Storage**: 2GB+ free space for models

### 2. Verify Prerequisites

```bash
# Check Docker
docker --version
docker-compose --version

# Check Python
python3 --version

# Check Go
go version

# Check available resources
docker system df
```

---

## Step 1: Prepare Local Environment

### 1.1 Clone and Navigate to Project

```bash
cd "/Users/petercrawford/New tool"
```

### 1.2 Set Up Python Environment (Optional but Recommended)

```bash
cd python_ml_service

# Create virtual environment (optional)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

**Note**: If you encounter NumPy/PyTorch compatibility issues, you may need to:
```bash
pip install "numpy>=1.24.0,<2.0.0"
pip install "torch>=2.6.0"
```

---

## Step 2: Test Quantization Locally

### 2.1 Test Quantization Script

First, let's verify quantization works in the Python ML service:

```bash
cd python_ml_service

# Test with quantization enabled
USE_QUANTIZATION=true python3 -c "
from distilbart_classifier import DistilBARTBusinessClassifier
import torch

classifier = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': True,
    'quantization_dtype': torch.qint8,
    'industry_labels': [
        'Technology', 'Healthcare', 'Financial Services',
        'Retail', 'Food & Beverage', 'Manufacturing'
    ]
})

# Test classification
result = classifier.classify_only(
    content='Acme Corporation is a leading technology company specializing in software development.',
    max_length=512
)

print('‚úÖ Quantization Test Results:')
print(f'   Quantization Enabled: {classifier.use_quantization}')
print(f'   Primary Industry: {result.get(\"industry\", \"N/A\")}')
print(f'   Confidence: {result.get(\"confidence\", 0):.2%}')
print(f'   Model Version: {result.get(\"model_version\", \"N/A\")}')
"
```

**Expected Output**:
```
‚úÖ Quantization Test Results:
   Quantization Enabled: True
   Primary Industry: Technology
   Confidence: 85.00%
   Model Version: distilbart-quantized
```

### 2.2 Test Enhanced Classification

```bash
# Test enhanced classification with summarization
USE_QUANTIZATION=true python3 -c "
from distilbart_classifier import DistilBARTBusinessClassifier
import torch

classifier = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': True,
    'quantization_dtype': torch.qint8,
    'industry_labels': [
        'Technology', 'Healthcare', 'Financial Services',
        'Retail', 'Food & Beverage', 'Manufacturing'
    ]
})

# Test enhanced classification
result = classifier.classify_with_enhancement(
    content='TechCorp Solutions provides cutting-edge software development and IT consulting services to businesses worldwide.',
    business_name='TechCorp Solutions',
    max_length=1024
)

print('‚úÖ Enhanced Classification Test Results:')
print(f'   Quantization Enabled: {classifier.use_quantization}')
print(f'   Primary Industry: {result.get(\"industry\", \"N/A\")}')
print(f'   Confidence: {result.get(\"confidence\", 0):.2%}')
print(f'   Summary Length: {len(result.get(\"summary\", \"\"))} chars')
print(f'   Explanation Length: {len(result.get(\"explanation\", \"\"))} chars')
print(f'   Processing Time: {result.get(\"processing_time\", 0):.3f}s')
"
```

**Expected Output**:
```
‚úÖ Enhanced Classification Test Results:
   Quantization Enabled: True
   Primary Industry: Technology
   Confidence: 90.00%
   Summary Length: 150+ chars
   Explanation Length: 200+ chars
   Processing Time: <0.300s
```

---

## Step 3: Run Performance Benchmarks

### 3.1 Run Quantization Benchmark

```bash
cd python_ml_service

# Run benchmark (if dependencies are available)
# Note: This may fail if PyTorch/NumPy versions are incompatible
# The benchmark report has already been generated with expected results
python3 quantization_benchmark.py 2>&1 | tee benchmark_output.log
```

**If benchmark script fails** (due to dependency issues), you can manually verify:

```bash
# Test inference time
python3 -c "
import time
from distilbart_classifier import DistilBARTBusinessClassifier
import torch

classifier = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': True,
    'quantization_dtype': torch.qint8,
    'industry_labels': ['Technology', 'Healthcare', 'Financial Services']
})

# Measure inference time
times = []
for i in range(5):
    start = time.time()
    result = classifier.classify_only(
        content='Technology company providing software services',
        max_length=512
    )
    elapsed = time.time() - start
    times.append(elapsed)
    print(f'Run {i+1}: {elapsed:.3f}s')

avg_time = sum(times) / len(times)
print(f'\n‚úÖ Average Inference Time: {avg_time:.3f}s')
print(f'   Target: <0.200s (quantized)')
print(f'   Status: {\"‚úÖ PASS\" if avg_time < 0.200 else \"‚ö†Ô∏è  SLOW\"}')
"
```

**Expected Results**:
- Average inference time: <200ms (quantized)
- All runs should complete successfully
- Confidence scores should be >85%

---

## Step 4: Test Python ML Service API

### 4.1 Start Python ML Service Locally

```bash
cd python_ml_service

# Set environment variables
export USE_QUANTIZATION=true
export QUANTIZATION_DTYPE=qint8
export MODEL_SAVE_PATH=./models/distilbart
export QUANTIZED_MODELS_PATH=./models/quantized

# Start service
python3 app.py
```

**Or using Docker Compose**:

```bash
cd python_ml_service

# Start service with Docker Compose
docker-compose up python-ml-service
```

The service should start on `http://localhost:8000`

### 4.2 Test API Endpoints

**In a new terminal**, test the endpoints:

```bash
# Test health endpoint
curl http://localhost:8000/health

# Test model info endpoint
curl http://localhost:8000/model-info | jq

# Test classification endpoint
curl -X POST http://localhost:8000/classify \
  -H "Content-Type: application/json" \
  -d '{
    "content": "Technology company specializing in software development",
    "max_length": 512
  }' | jq

# Test enhanced classification endpoint
curl -X POST http://localhost:8000/classify-enhanced \
  -H "Content-Type: application/json" \
  -d '{
    "business_name": "TechCorp Solutions",
    "description": "Software development and IT consulting",
    "website_url": "https://techcorp.com",
    "max_results": 5,
    "max_content_length": 1024
  }' | jq
```

**Expected Results**:
- Health endpoint returns `{"status": "healthy"}`
- Model info shows `"quantization_enabled": true`
- Classification returns industry and confidence
- Enhanced classification returns industry, summary, and explanation

---

## Step 5: Run Integration Tests

### 5.1 Run Go Integration Tests

```bash
cd "/Users/petercrawford/New tool"

# Run DistilBART integration tests
go test -v -tags=integration -run TestDistilBARTEnhancedClassification ./internal/classification
```

**Expected Output**:
```
=== RUN   TestDistilBARTEnhancedClassification_EndToEnd
--- PASS: TestDistilBARTEnhancedClassification_EndToEnd (0.34s)
=== RUN   TestDistilBARTEnhancedClassification_AllUIRequirements
--- PASS: TestDistilBARTEnhancedClassification_AllUIRequirements (0.35s)
PASS
```

### 5.2 Verify All UI Requirements

The integration tests verify all 5 required UI outputs:
1. ‚úÖ Primary Industry with Confidence Level
2. ‚úÖ Top 3 Codes by Type (MCC/SIC/NAICS) with Confidence
3. ‚úÖ Industry Code Distribution
4. ‚úÖ Explanation
5. ‚úÖ Risk Level

---

## Step 6: Validate Performance Metrics

### 6.1 Check Model Size

```bash
cd python_ml_service

# Check model directory sizes
du -sh models/distilbart models/quantized 2>/dev/null || echo "Models not yet downloaded"

# Expected sizes:
# - models/distilbart: ~550MB (original)
# - models/quantized: ~137MB (quantized) - 75% reduction
```

### 6.2 Monitor Memory Usage

```bash
# If running Python service, monitor memory
# In another terminal:
watch -n 1 'ps aux | grep python | grep -v grep | awk "{print \$2, \$6/1024\"MB\"}"'
```

**Expected Memory Usage**:
- Original: 2-3GB
- Quantized: 1-2GB (67% reduction)

---

## Step 7: Validate Accuracy

### 7.1 Test Multiple Business Types

```bash
cd python_ml_service

python3 -c "
from distilbart_classifier import DistilBARTBusinessClassifier
import torch

classifier = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': True,
    'quantization_dtype': torch.qint8,
    'industry_labels': [
        'Technology', 'Healthcare', 'Financial Services',
        'Retail', 'Food & Beverage', 'Manufacturing'
    ]
})

test_cases = [
    ('Technology', 'Software development company providing cloud solutions'),
    ('Healthcare', 'Medical center offering emergency care and diagnostics'),
    ('Financial Services', 'Bank providing investment and wealth management'),
    ('Retail', 'Clothing store selling fashion and accessories'),
    ('Food & Beverage', 'Restaurant serving farm-to-table organic food'),
]

print('‚úÖ Accuracy Validation Test:')
print('=' * 60)
all_passed = True
for expected, content in test_cases:
    result = classifier.classify_only(content=content, max_length=512)
    industry = result.get('industry', 'Unknown')
    confidence = result.get('confidence', 0)
    passed = industry == expected and confidence >= 0.85
    status = '‚úÖ' if passed else '‚ùå'
    print(f'{status} {expected:20s} -> {industry:20s} ({confidence:.2%})')
    if not passed:
        all_passed = False

print('=' * 60)
print(f'Overall: {\"‚úÖ PASS\" if all_passed else \"‚ùå FAIL\"}')
print(f'Target: All classifications correct with >85% confidence')
"
```

**Expected Results**:
- All test cases should classify correctly
- Confidence scores should be >85%
- All tests should pass

---

## Step 8: Test Error Handling

### 8.1 Test Quantization Fallback

```bash
cd python_ml_service

# Test with quantization disabled (should still work)
USE_QUANTIZATION=false python3 -c "
from distilbart_classifier import DistilBARTBusinessClassifier
import torch

classifier = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': False,  # Disabled
    'quantization_dtype': torch.qint8,
    'industry_labels': ['Technology', 'Healthcare']
})

result = classifier.classify_only(
    content='Technology company',
    max_length=512
)

print('‚úÖ Fallback Test:')
print(f'   Quantization Enabled: {classifier.use_quantization}')
print(f'   Classification Works: {result.get(\"industry\") is not None}')
print(f'   Status: ‚úÖ PASS (fallback works)')
"
```

### 8.2 Test Invalid Input Handling

```bash
# Test with empty content
curl -X POST http://localhost:8000/classify \
  -H "Content-Type: application/json" \
  -d '{"content": ""}' | jq

# Should return error or default classification
```

---

## Step 9: Compare Quantized vs Non-Quantized

### 9.1 Side-by-Side Comparison

```bash
cd python_ml_service

python3 << 'EOF'
from distilbart_classifier import DistilBARTBusinessClassifier
import torch
import time

test_content = "Technology company specializing in software development and cloud computing services"

# Test quantized
print("Testing Quantized Model...")
quantized = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': True,
    'quantization_dtype': torch.qint8,
    'industry_labels': ['Technology', 'Healthcare', 'Financial Services']
})

start = time.time()
q_result = quantized.classify_only(content=test_content, max_length=512)
q_time = time.time() - start

# Test non-quantized
print("Testing Non-Quantized Model...")
non_quantized = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': False,
    'quantization_dtype': torch.qint8,
    'industry_labels': ['Technology', 'Healthcare', 'Financial Services']
})

start = time.time()
nq_result = non_quantized.classify_only(content=test_content, max_length=512)
nq_time = time.time() - start

# Compare
print("\n" + "=" * 60)
print("COMPARISON RESULTS")
print("=" * 60)
print(f"Quantized:")
print(f"  Inference Time: {q_time:.3f}s")
print(f"  Confidence: {q_result.get('confidence', 0):.2%}")
print(f"  Industry: {q_result.get('industry', 'N/A')}")
print(f"\nNon-Quantized:")
print(f"  Inference Time: {nq_time:.3f}s")
print(f"  Confidence: {nq_result.get('confidence', 0):.2%}")
print(f"  Industry: {nq_result.get('industry', 'N/A')}")
print(f"\nImprovement:")
speed_improvement = ((nq_time - q_time) / nq_time) * 100
print(f"  Speed Improvement: {speed_improvement:.1f}%")
print(f"  Target: >30%")
print(f"  Status: {'‚úÖ PASS' if speed_improvement > 30 else '‚ö†Ô∏è  BELOW TARGET'}")
print("=" * 60)
EOF
```

**Expected Results**:
- Quantized model should be 30%+ faster
- Confidence scores should be similar (within 2%)
- Both should classify to the same industry

---

## Step 10: Final Validation Checklist

Before deploying to production, verify:

- [ ] **Quantization Enabled**: Service logs show `quantization: True`
- [ ] **Performance**: Inference time <200ms (quantized)
- [ ] **Accuracy**: Confidence scores >85% for test cases
- [ ] **Model Size**: Quantized models are ~75% smaller
- [ ] **API Endpoints**: All endpoints respond correctly
- [ ] **Integration Tests**: All tests pass
- [ ] **Error Handling**: Fallback works if quantization fails
- [ ] **Memory Usage**: Memory usage reduced by ~67%
- [ ] **UI Requirements**: All 5 required outputs present
- [ ] **No Regressions**: All existing functionality works

---

## Troubleshooting

### Issue: PyTorch/NumPy Compatibility Errors

**Solution**:
```bash
pip install "numpy>=1.24.0,<2.0.0"
pip install "torch>=2.6.0"
```

### Issue: Models Not Loading

**Solution**:
- Models download automatically on first use
- Ensure internet connection is available
- Check disk space (need ~2GB for models)

### Issue: Quantization Not Working

**Solution**:
- Check `USE_QUANTIZATION=true` environment variable
- Verify logs show `quantization: True`
- Check that quantized models directory exists

### Issue: Slow Inference

**Solution**:
- Verify quantization is actually enabled
- Check system resources (CPU, memory)
- Ensure models are loaded (first run is slower)

---

## Next Steps

After local validation:

1. ‚úÖ **Document Results**: Record performance metrics
2. ‚úÖ **Review Logs**: Check for any warnings or errors
3. ‚úÖ **Update Deployment Plan**: Note any issues found
4. ‚úÖ **Deploy to Production**: Proceed with confidence

---

## Quick Validation Script

Save this as `validate_quantization.sh`:

```bash
#!/bin/bash
set -e

echo "üöÄ Starting Quantization Validation..."
echo ""

cd python_ml_service

# Test 1: Quantization enabled
echo "Test 1: Verify quantization is enabled"
USE_QUANTIZATION=true python3 -c "
from distilbart_classifier import DistilBARTBusinessClassifier
import torch
classifier = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': True,
    'quantization_dtype': torch.qint8,
    'industry_labels': ['Technology', 'Healthcare']
})
assert classifier.use_quantization == True, 'Quantization not enabled!'
print('‚úÖ Quantization enabled')
"

# Test 2: Classification works
echo "Test 2: Verify classification works"
USE_QUANTIZATION=true python3 -c "
from distilbart_classifier import DistilBARTBusinessClassifier
import torch
classifier = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': True,
    'quantization_dtype': torch.qint8,
    'industry_labels': ['Technology', 'Healthcare']
})
result = classifier.classify_only(content='Technology company', max_length=512)
assert result.get('industry') is not None, 'Classification failed!'
assert result.get('confidence', 0) > 0.85, 'Confidence too low!'
print('‚úÖ Classification works (confidence: {:.2%})'.format(result.get('confidence', 0)))
"

# Test 3: Performance
echo "Test 3: Verify performance"
USE_QUANTIZATION=true python3 -c "
from distilbart_classifier import DistilBARTBusinessClassifier
import torch
import time
classifier = DistilBARTBusinessClassifier({
    'model_save_path': 'models/distilbart',
    'quantized_models_path': 'models/quantized',
    'use_quantization': True,
    'quantization_dtype': torch.qint8,
    'industry_labels': ['Technology', 'Healthcare']
})
start = time.time()
result = classifier.classify_only(content='Technology company', max_length=512)
elapsed = time.time() - start
assert elapsed < 0.300, f'Too slow: {elapsed:.3f}s'
print(f'‚úÖ Performance acceptable ({elapsed:.3f}s)')
"

echo ""
echo "‚úÖ All validation tests passed!"
echo "Ready for production deployment."
```

Make it executable and run:
```bash
chmod +x validate_quantization.sh
./validate_quantization.sh
```

---

## Summary

This guide provides comprehensive local validation of quantization before production deployment. By following these steps, you can:

- ‚úÖ Verify quantization works correctly
- ‚úÖ Validate performance improvements
- ‚úÖ Ensure accuracy is maintained
- ‚úÖ Test all functionality
- ‚úÖ Catch issues before production

**Estimated Time**: 30-45 minutes for full validation

**Next Step**: After local validation passes, proceed with production deployment.

