package enrichment

import (
	"context"
	"fmt"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"go.uber.org/zap"
)

// Test data structures
type TestDataSource struct {
	ID       string    `json:"id"`
	Name     string    `json:"name"`
	Type     string    `json:"type"`
	CreatedAt time.Time `json:"created_at"`
	UpdatedAt time.Time `json:"updated_at"`
}

func TestNewDataSourceReliabilityAssessor(t *testing.T) {
	tests := []struct {
		name   string
		config *DataSourceReliabilityConfig
	}{
		{
			name:   "with nil config",
			config: nil,
		},
		{
			name: "with custom config",
			config: &DataSourceReliabilityConfig{
				EnableHistoricalAnalysis: true,
				PerformanceThreshold:     1 * time.Second,
				UptimeThreshold:          99.0,
				HistoricalWeight:         0.3,
				PerformanceWeight:        0.3,
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), tt.config)
			assert.NotNil(t, assessor)
			assert.NotNil(t, assessor.config)
			assert.NotNil(t, assessor.logger)
			assert.NotNil(t, assessor.tracer)

			if tt.config == nil {
				// Should use default config
				assert.True(t, assessor.config.EnableHistoricalAnalysis)
				assert.Equal(t, 2*time.Second, assessor.config.PerformanceThreshold)
				assert.Equal(t, 95.0, assessor.config.UptimeThreshold)
			} else {
				// Should use provided config
				assert.Equal(t, tt.config.EnableHistoricalAnalysis, assessor.config.EnableHistoricalAnalysis)
				assert.Equal(t, tt.config.PerformanceThreshold, assessor.config.PerformanceThreshold)
				assert.Equal(t, tt.config.UptimeThreshold, assessor.config.UptimeThreshold)
			}
		})
	}
}

func TestDataSourceReliabilityAssessor_AssessSourceReliability(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)
	ctx := context.Background()

	tests := []struct {
		name       string
		sourceID   string
		sourceType string
		sourceName string
		data       interface{}
	}{
		{
			name:       "assess reliable source",
			sourceID:   "test-reliable",
			sourceType: "api",
			sourceName: "Reliable API",
			data: &TestDataSource{
				ID:        "test-reliable",
				Name:      "Reliable API",
				Type:      "api",
				CreatedAt: time.Now().Add(-24 * time.Hour),
				UpdatedAt: time.Now().Add(-1 * time.Hour),
			},
		},
		{
			name:       "assess unreliable source",
			sourceID:   "test-unreliable",
			sourceType: "website",
			sourceName: "Unreliable Website",
			data: &TestDataSource{
				ID:        "test-unreliable",
				Name:      "Unreliable Website",
				Type:      "website",
				CreatedAt: time.Now().Add(-24 * time.Hour),
				UpdatedAt: time.Now().Add(-12 * time.Hour),
			},
		},
		{
			name:       "assess source with nil data",
			sourceID:   "test-nil",
			sourceType: "database",
			sourceName: "Database Source",
			data:       nil,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			result, err := assessor.AssessSourceReliability(ctx, tt.sourceID, tt.sourceType, tt.sourceName, tt.data)
			require.NoError(t, err)
			require.NotNil(t, result)

			// Basic result validation
			assert.Equal(t, tt.sourceID, result.SourceID)
			assert.NotNil(t, result.Assessment)
			assert.Equal(t, tt.sourceID, result.Assessment.SourceID)
			assert.Equal(t, tt.sourceType, result.Assessment.SourceType)
			assert.Equal(t, tt.sourceName, result.Assessment.SourceName)

			// Score validation
			assert.GreaterOrEqual(t, result.OverallScore, 0.0)
			assert.LessOrEqual(t, result.OverallScore, 1.0)
			assert.GreaterOrEqual(t, result.Assessment.OverallReliability, 0.0)
			assert.LessOrEqual(t, result.Assessment.OverallReliability, 1.0)

			// Reliability level validation
			assert.Contains(t, []string{"excellent", "good", "fair", "poor", "critical"}, result.ReliabilityLevel)
			assert.Contains(t, []string{"excellent", "good", "fair", "poor", "critical"}, result.Assessment.ReliabilityLevel)

			// Component scores validation
			assert.GreaterOrEqual(t, result.Assessment.HistoricalScore, 0.0)
			assert.LessOrEqual(t, result.Assessment.HistoricalScore, 1.0)
			assert.GreaterOrEqual(t, result.Assessment.PerformanceScore, 0.0)
			assert.LessOrEqual(t, result.Assessment.PerformanceScore, 1.0)
			assert.GreaterOrEqual(t, result.Assessment.AccuracyScore, 0.0)
			assert.LessOrEqual(t, result.Assessment.AccuracyScore, 1.0)
			assert.GreaterOrEqual(t, result.Assessment.ConsistencyScore, 0.0)
			assert.LessOrEqual(t, result.Assessment.ConsistencyScore, 1.0)
			assert.GreaterOrEqual(t, result.Assessment.UptimeScore, 0.0)
			assert.LessOrEqual(t, result.Assessment.UptimeScore, 1.0)
			assert.GreaterOrEqual(t, result.Assessment.DataQualityScore, 0.0)
			assert.LessOrEqual(t, result.Assessment.DataQualityScore, 1.0)

			// Metadata validation
			assert.NotZero(t, result.AssessedAt)
			assert.Greater(t, result.ProcessingTime, time.Duration(0))
			assert.GreaterOrEqual(t, result.DataPoints, 0)

			// Risk assessment validation
			assert.NotNil(t, result.RiskAssessment)
			assert.Contains(t, []string{"low", "medium", "high", "critical"}, result.RiskLevel)
			assert.Contains(t, []string{"low", "medium", "high", "critical"}, result.RiskAssessment.RiskLevel)
			assert.GreaterOrEqual(t, result.RiskAssessment.RiskScore, 0.0)
			assert.LessOrEqual(t, result.RiskAssessment.RiskScore, 1.0)

			// Recommendations validation
			assert.NotNil(t, result.Recommendations)
			assert.NotNil(t, result.PriorityActions)
		},
	}
}

func TestDataSourceReliabilityAssessor_RecordPerformance(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)
	ctx := context.Background()

	tests := []struct {
		name             string
		sourceID         string
		responseTime     time.Duration
		success          bool
		errorType        string
		dataQualityScore float64
	}{
		{
			name:             "record successful performance",
			sourceID:         "test-success",
			responseTime:     500 * time.Millisecond,
			success:          true,
			errorType:        "",
			dataQualityScore: 0.9,
		},
		{
			name:             "record failed performance",
			sourceID:         "test-failure",
			responseTime:     2 * time.Second,
			success:          false,
			errorType:        "timeout",
			dataQualityScore: 0.3,
		},
		{
			name:             "record slow performance",
			sourceID:         "test-slow",
			responseTime:     3 * time.Second,
			success:          true,
			errorType:        "",
			dataQualityScore: 0.7,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			err := assessor.RecordPerformance(ctx, tt.sourceID, tt.responseTime, tt.success, tt.errorType, tt.dataQualityScore)
			require.NoError(t, err)

			// Verify metrics were recorded
			metrics, err := assessor.GetReliabilityMetrics(ctx, tt.sourceID)
			require.NoError(t, err)
			assert.Equal(t, tt.sourceID, metrics.SourceID)
			assert.Equal(t, int64(1), metrics.TotalRequests)

			if tt.success {
				assert.Equal(t, int64(1), metrics.SuccessfulRequests)
				assert.Equal(t, int64(0), metrics.FailedRequests)
				assert.Equal(t, 100.0, metrics.UptimePercentage)
				assert.Equal(t, 0.0, metrics.ErrorRate)
			} else {
				assert.Equal(t, int64(0), metrics.SuccessfulRequests)
				assert.Equal(t, int64(1), metrics.FailedRequests)
				assert.Equal(t, 0.0, metrics.UptimePercentage)
				assert.Equal(t, 1.0, metrics.ErrorRate)
			}

			assert.Equal(t, tt.responseTime, metrics.AverageResponseTime)
			assert.NotZero(t, metrics.LastUpdated)
		}
	}
}

func TestDataSourceReliabilityAssessor_RecordPerformanceMultiple(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)
	ctx := context.Background()

	sourceID := "test-multiple"

	// Record multiple performance metrics
	performances := []struct {
		responseTime     time.Duration
		success          bool
		errorType        string
		dataQualityScore float64
	}{
		{500 * time.Millisecond, true, "", 0.9},
		{600 * time.Millisecond, true, "", 0.8},
		{2 * time.Second, false, "timeout", 0.3},
		{400 * time.Millisecond, true, "", 0.95},
	}

	for _, perf := range performances {
		err := assessor.RecordPerformance(ctx, sourceID, perf.responseTime, perf.success, perf.errorType, perf.dataQualityScore)
		require.NoError(t, err)
	}

	// Verify aggregated metrics
	metrics, err := assessor.GetReliabilityMetrics(ctx, sourceID)
	require.NoError(t, err)
	assert.Equal(t, sourceID, metrics.SourceID)
	assert.Equal(t, int64(4), metrics.TotalRequests)
	assert.Equal(t, int64(3), metrics.SuccessfulRequests)
	assert.Equal(t, int64(1), metrics.FailedRequests)
	assert.Equal(t, 75.0, metrics.UptimePercentage)
	assert.Equal(t, 0.25, metrics.ErrorRate)
	assert.NotZero(t, metrics.AverageResponseTime)
}

func TestDataSourceReliabilityAssessor_GetSourceAssessment(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)
	ctx := context.Background()

	// First assess a source
	sourceID := "test-get-assessment"
	data := &TestDataSource{
		ID:        sourceID,
		Name:      "Test Source",
		Type:      "api",
		CreatedAt: time.Now().Add(-24 * time.Hour),
		UpdatedAt: time.Now().Add(-1 * time.Hour),
	}

	_, err := assessor.AssessSourceReliability(ctx, sourceID, "api", "Test Source", data)
	require.NoError(t, err)

	// Then retrieve the assessment
	assessment, err := assessor.GetSourceAssessment(ctx, sourceID)
	require.NoError(t, err)
	assert.NotNil(t, assessment)
	assert.Equal(t, sourceID, assessment.SourceID)
	assert.Equal(t, "api", assessment.SourceType)
	assert.Equal(t, "Test Source", assessment.SourceName)

	// Test getting non-existent assessment
	_, err = assessor.GetSourceAssessment(ctx, "non-existent")
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "no assessment found")
}

func TestDataSourceReliabilityAssessor_GetReliabilityMetrics(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)
	ctx := context.Background()

	sourceID := "test-get-metrics"

	// Record some performance data
	err := assessor.RecordPerformance(ctx, sourceID, 500*time.Millisecond, true, "", 0.9)
	require.NoError(t, err)

	// Retrieve metrics
	metrics, err := assessor.GetReliabilityMetrics(ctx, sourceID)
	require.NoError(t, err)
	assert.NotNil(t, metrics)
	assert.Equal(t, sourceID, metrics.SourceID)
	assert.Equal(t, int64(1), metrics.TotalRequests)

	// Test getting non-existent metrics
	_, err = assessor.GetReliabilityMetrics(ctx, "non-existent")
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "no metrics found")
}

func TestDataSourceReliabilityAssessor_GetHistoricalPerformance(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)
	ctx := context.Background()

	sourceID := "test-get-history"

	// Record some performance data
	err := assessor.RecordPerformance(ctx, sourceID, 500*time.Millisecond, true, "", 0.9)
	require.NoError(t, err)

	// Retrieve historical performance
	history, err := assessor.GetHistoricalPerformance(ctx, sourceID)
	require.NoError(t, err)
	assert.NotNil(t, history)
	assert.Equal(t, sourceID, history.SourceID)
	assert.Equal(t, 1, len(history.PerformanceMetrics))

	// Test getting non-existent history
	_, err = assessor.GetHistoricalPerformance(ctx, "non-existent")
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "no historical data found")
}

func TestDataSourceReliabilityAssessor_ReliabilityLevelDetermination(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)

	tests := []struct {
		name           string
		score          float64
		expectedLevel  string
	}{
		{
			name:          "excellent score",
			score:         0.95,
			expectedLevel: "excellent",
		},
		{
			name:          "good score",
			score:         0.85,
			expectedLevel: "good",
		},
		{
			name:          "fair score",
			score:         0.75,
			expectedLevel: "fair",
		},
		{
			name:          "poor score",
			score:         0.6,
			expectedLevel: "poor",
		},
		{
			name:          "critical score",
			score:         0.3,
			expectedLevel: "critical",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			level := assessor.determineReliabilityLevel(tt.score)
			assert.Equal(t, tt.expectedLevel, level)
		})
	}
}

func TestDataSourceReliabilityAssessor_RiskLevelDetermination(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)

	tests := []struct {
		name           string
		assessment     *SourceAssessment
		expectedLevel  string
	}{
		{
			name: "low risk assessment",
			assessment: &SourceAssessment{
				ReliabilityLevel: "excellent",
				UptimePercentage: 99.5,
				ErrorRate:        0.01,
			},
			expectedLevel: "low",
		},
		{
			name: "medium risk assessment",
			assessment: &SourceAssessment{
				ReliabilityLevel: "fair",
				UptimePercentage: 92.0,
				ErrorRate:        0.05,
			},
			expectedLevel: "medium",
		},
		{
			name: "high risk assessment",
			assessment: &SourceAssessment{
				ReliabilityLevel: "poor",
				UptimePercentage: 85.0,
				ErrorRate:        0.15,
			},
			expectedLevel: "high",
		},
		{
			name: "critical risk assessment",
			assessment: &SourceAssessment{
				ReliabilityLevel: "critical",
				UptimePercentage: 70.0,
				ErrorRate:        0.3,
			},
			expectedLevel: "critical",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			level := assessor.determineRiskLevel(tt.assessment)
			assert.Equal(t, tt.expectedLevel, level)
		})
	}
}

func TestDataSourceReliabilityAssessor_PerformanceScoring(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)

	tests := []struct {
		name                string
		metrics             *ReliabilityMetrics
		expectedScoreMin    float64
		expectedScoreMax    float64
	}{
		{
			name: "excellent performance",
			metrics: &ReliabilityMetrics{
				UptimePercentage:    99.5,
				AverageResponseTime: 500 * time.Millisecond,
				ErrorRate:           0.01,
			},
			expectedScoreMin: 0.9,
			expectedScoreMax: 1.0,
		},
		{
			name: "good performance",
			metrics: &ReliabilityMetrics{
				UptimePercentage:    95.0,
				AverageResponseTime: 1 * time.Second,
				ErrorRate:           0.05,
			},
			expectedScoreMin: 0.7,
			expectedScoreMax: 0.9,
		},
		{
			name: "poor performance",
			metrics: &ReliabilityMetrics{
				UptimePercentage:    80.0,
				AverageResponseTime: 5 * time.Second,
				ErrorRate:           0.2,
			},
			expectedScoreMin: 0.0,
			expectedScoreMax: 0.5,
		},
		{
			name:                "nil metrics",
			metrics:             nil,
			expectedScoreMin:    0.7,
			expectedScoreMax:    0.7,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			score := assessor.calculatePerformanceScore(tt.metrics)
			assert.GreaterOrEqual(t, score, tt.expectedScoreMin)
			assert.LessOrEqual(t, score, tt.expectedScoreMax)
		})
	}
}

func TestDataSourceReliabilityAssessor_ConsistencyScoring(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)

	tests := []struct {
		name                string
		sourceID            string
		responseTimes       []time.Duration
		expectedConsistency float64
	}{
		{
			name:          "high consistency",
			sourceID:      "test-consistent",
			responseTimes: []time.Duration{500 * time.Millisecond, 520 * time.Millisecond, 480 * time.Millisecond},
			expectedConsistency: 0.8,
		},
		{
			name:          "low consistency",
			sourceID:      "test-inconsistent",
			responseTimes: []time.Duration{100 * time.Millisecond, 2 * time.Second, 500 * time.Millisecond},
			expectedConsistency: 0.0,
		},
		{
			name:                "no history",
			sourceID:            "test-no-history",
			responseTimes:       []time.Duration{},
			expectedConsistency: 0.7,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Record performance metrics
			for _, responseTime := range tt.responseTimes {
				err := assessor.RecordPerformance(context.Background(), tt.sourceID, responseTime, true, "", 0.8)
				require.NoError(t, err)
			}

			// Calculate consistency score
			consistency := assessor.calculateConsistencyScore(tt.sourceID)
			assert.GreaterOrEqual(t, consistency, 0.0)
			assert.LessOrEqual(t, consistency, 1.0)

			if len(tt.responseTimes) > 0 {
				assert.GreaterOrEqual(t, consistency, tt.expectedConsistency)
			}
		})
	}
}

func TestDataSourceReliabilityAssessor_TrendAnalysis(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)

	tests := []struct {
		name                string
		assessments         []*SourceAssessment
		expectedDirection   string
		expectedConfidence  float64
	}{
		{
			name: "improving trend",
			assessments: []*SourceAssessment{
				{OverallReliability: 0.6},
				{OverallReliability: 0.7},
				{OverallReliability: 0.8},
			},
			expectedDirection:  "improving",
			expectedConfidence: 0.3,
		},
		{
			name: "declining trend",
			assessments: []*SourceAssessment{
				{OverallReliability: 0.9},
				{OverallReliability: 0.8},
				{OverallReliability: 0.7},
			},
			expectedDirection:  "declining",
			expectedConfidence: 0.3,
		},
		{
			name: "stable trend",
			assessments: []*SourceAssessment{
				{OverallReliability: 0.8},
				{OverallReliability: 0.82},
				{OverallReliability: 0.78},
			},
			expectedDirection:  "stable",
			expectedConfidence: 0.3,
		},
		{
			name:                "insufficient data",
			assessments:         []*SourceAssessment{},
			expectedDirection:   "stable",
			expectedConfidence:  0.3,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			history := &PerformanceHistory{
				SourceID:    "test-trend",
				Assessments: tt.assessments,
			}

			trend := assessor.analyzeTrend(history)
			assert.Equal(t, tt.expectedDirection, trend.Direction)
			assert.GreaterOrEqual(t, trend.Confidence, 0.0)
			assert.LessOrEqual(t, trend.Confidence, 1.0)
		}
	}
}

func TestDataSourceReliabilityAssessor_RiskAssessment(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)

	tests := []struct {
		name                string
		assessment          *SourceAssessment
		metrics             *ReliabilityMetrics
		expectedRiskLevel   string
		expectedRiskFactors int
	}{
		{
			name: "low risk",
			assessment: &SourceAssessment{
				ReliabilityLevel: "excellent",
				UptimePercentage: 99.5,
				ErrorRate:        0.01,
				AverageResponseTime: 500 * time.Millisecond,
			},
			metrics: &ReliabilityMetrics{
				UptimePercentage: 99.5,
				ErrorRate:        0.01,
			},
			expectedRiskLevel:   "low",
			expectedRiskFactors: 0,
		},
		{
			name: "high risk",
			assessment: &SourceAssessment{
				ReliabilityLevel: "poor",
				UptimePercentage: 85.0,
				ErrorRate:        0.15,
				AverageResponseTime: 3 * time.Second,
			},
			metrics: &ReliabilityMetrics{
				UptimePercentage: 85.0,
				ErrorRate:        0.15,
			},
			expectedRiskLevel:   "high",
			expectedRiskFactors: 3,
		},
		{
			name: "critical risk",
			assessment: &SourceAssessment{
				ReliabilityLevel: "critical",
				UptimePercentage: 70.0,
				ErrorRate:        0.3,
				AverageResponseTime: 5 * time.Second,
			},
			metrics: &ReliabilityMetrics{
				UptimePercentage: 70.0,
				ErrorRate:        0.3,
			},
			expectedRiskLevel:   "critical",
			expectedRiskFactors: 4,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			riskAssessment := assessor.assessRisk(tt.assessment, tt.metrics)
			assert.Equal(t, tt.expectedRiskLevel, riskAssessment.RiskLevel)
			assert.GreaterOrEqual(t, len(riskAssessment.RiskFactors), tt.expectedRiskFactors)
			assert.GreaterOrEqual(t, riskAssessment.RiskScore, 0.0)
			assert.LessOrEqual(t, riskAssessment.RiskScore, 1.0)
			assert.NotEmpty(t, riskAssessment.MitigationActions)
		}
	}
}

func TestDataSourceReliabilityAssessor_RecommendationGeneration(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)

	tests := []struct {
		name                    string
		assessment              *SourceAssessment
		riskAssessment          *RiskAssessment
		expectedRecommendations int
		expectedActions         int
	}{
		{
			name: "excellent source - few recommendations",
			assessment: &SourceAssessment{
				ReliabilityLevel: "excellent",
				UptimePercentage: 99.5,
				ErrorRate:        0.01,
				ConsistencyScore: 0.9,
			},
			riskAssessment: &RiskAssessment{
				RiskLevel: "low",
			},
			expectedRecommendations: 0,
			expectedActions:         0,
		},
		{
			name: "poor source - recommendations expected",
			assessment: &SourceAssessment{
				ReliabilityLevel: "poor",
				UptimePercentage: 85.0,
				ErrorRate:        0.15,
				ConsistencyScore: 0.6,
			},
			riskAssessment: &RiskAssessment{
				RiskLevel: "high",
			},
			expectedRecommendations: 2,
			expectedActions:         1,
		},
		{
			name: "critical source - urgent actions",
			assessment: &SourceAssessment{
				ReliabilityLevel: "critical",
				UptimePercentage: 70.0,
				ErrorRate:        0.3,
				ConsistencyScore: 0.4,
			},
			riskAssessment: &RiskAssessment{
				RiskLevel: "critical",
			},
			expectedRecommendations: 3,
			expectedActions:         2,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			recommendations := assessor.generateRecommendations(tt.assessment, tt.riskAssessment)
			actions := assessor.generatePriorityActions(tt.assessment, tt.riskAssessment)

			assert.GreaterOrEqual(t, len(recommendations), tt.expectedRecommendations)
			assert.GreaterOrEqual(t, len(actions), tt.expectedActions)
		}
	}
}

func TestDataSourceReliabilityAssessor_Performance(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)
	ctx := context.Background()

	// Performance test for assessment
	iterations := 100
	start := time.Now()

	for i := 0; i < iterations; i++ {
		data := &TestDataSource{
			ID:        fmt.Sprintf("perf-test-%d", i),
			Name:      fmt.Sprintf("Performance Test %d", i),
			Type:      "api",
			CreatedAt: time.Now().Add(-24 * time.Hour),
			UpdatedAt: time.Now().Add(-1 * time.Hour),
		}

		_, err := assessor.AssessSourceReliability(ctx, fmt.Sprintf("perf-test-%d", i), "api", fmt.Sprintf("Performance Test %d", i), data)
		require.NoError(t, err)
	}

	duration := time.Since(start)
	avgDuration := duration / time.Duration(iterations)

	// Performance assertions
	assert.Less(t, avgDuration, 10*time.Millisecond, "Average assessment should take less than 10ms")
	assert.Less(t, duration, 1*time.Second, "Total time for %d iterations should be less than 1 second", iterations)

	t.Logf("Assessment performance: %d iterations in %v (avg: %v per iteration)", 
		iterations, duration, avgDuration)

	// Performance test for recording
	start = time.Now()
	for i := 0; i < iterations; i++ {
		err := assessor.RecordPerformance(ctx, "perf-record", 500*time.Millisecond, true, "", 0.8)
		require.NoError(t, err)
	}

	duration = time.Since(start)
	avgDuration = duration / time.Duration(iterations)

	// Performance assertions
	assert.Less(t, avgDuration, 5*time.Millisecond, "Average recording should take less than 5ms")
	assert.Less(t, duration, 1*time.Second, "Total recording time for %d iterations should be less than 1 second", iterations)

	t.Logf("Recording performance: %d iterations in %v (avg: %v per iteration)", 
		iterations, duration, avgDuration)
}

func TestDataSourceReliabilityAssessor_Concurrency(t *testing.T) {
	assessor := NewDataSourceReliabilityAssessor(zap.NewNop(), nil)
	ctx := context.Background()

	// Test concurrent assessment and recording
	const numGoroutines = 10
	const iterationsPerGoroutine = 10

	done := make(chan bool, numGoroutines)

	for i := 0; i < numGoroutines; i++ {
		go func(id int) {
			defer func() { done <- true }()

			sourceID := fmt.Sprintf("concurrent-test-%d", id)
			data := &TestDataSource{
				ID:        sourceID,
				Name:      fmt.Sprintf("Concurrent Test %d", id),
				Type:      "api",
				CreatedAt: time.Now().Add(-24 * time.Hour),
				UpdatedAt: time.Now().Add(-1 * time.Hour),
			}

			for j := 0; j < iterationsPerGoroutine; j++ {
				// Assess source
				_, err := assessor.AssessSourceReliability(ctx, sourceID, "api", fmt.Sprintf("Concurrent Test %d", id), data)
				require.NoError(t, err)

				// Record performance
				err = assessor.RecordPerformance(ctx, sourceID, 500*time.Millisecond, true, "", 0.8)
				require.NoError(t, err)
			}
		}(i)
	}

	// Wait for all goroutines to complete
	for i := 0; i < numGoroutines; i++ {
		<-done
	}

	// Verify that all data was recorded
	for i := 0; i < numGoroutines; i++ {
		sourceID := fmt.Sprintf("concurrent-test-%d", i)
		
		// Check assessment
		assessment, err := assessor.GetSourceAssessment(ctx, sourceID)
		require.NoError(t, err)
		assert.Equal(t, sourceID, assessment.SourceID)

		// Check metrics
		metrics, err := assessor.GetReliabilityMetrics(ctx, sourceID)
		require.NoError(t, err)
		assert.Equal(t, int64(iterationsPerGoroutine), metrics.TotalRequests)
	}
}
