package middleware

import (
	"context"
	"crypto/rand"
	"fmt"
	"log"
	"math"
	"net/http"
	"sync/atomic"
	"time"

	"github.com/shirou/gopsutil/v3/cpu"
	"github.com/shirou/gopsutil/v3/mem"
)

// DefaultHorizontalScalingConfig returns default configuration
func DefaultHorizontalScalingConfig() *HorizontalScalingConfig {
	return &HorizontalScalingConfig{
		LoadBalancingEnabled:    true,
		ServiceDiscoveryEnabled: true,
		HealthCheckEnabled:      true,
		AutoScalingEnabled:      true,
		SessionAffinityEnabled:  true,
		StickySessionTimeout:    30 * time.Minute,
		MaxInstances:            10,
		MinInstances:            2,
		ScalingCooldown:         5 * time.Minute,
		HealthCheckInterval:     30 * time.Second,
		LoadBalancingStrategy:   "round-robin",
		LoadBalancerConfig: &LoadBalancerConfig{
			Strategy:              "round-robin",
			HealthCheckPath:       "/health",
			HealthCheckTimeout:    5 * time.Second,
			MaxRetries:            3,
			RetryTimeout:          10 * time.Second,
			SessionAffinity:       true,
			SessionTimeout:        30 * time.Minute,
			ConnectionPooling:     true,
			MaxConnectionsPerHost: 100,
			KeepAliveTimeout:      30 * time.Second,
		},
		ServiceDiscoveryConfig: &ServiceDiscoveryConfig{
			RegistrationEnabled:  true,
			RegistrationInterval: 30 * time.Second,
			DiscoveryInterval:    60 * time.Second,
			ServiceRegistryURL:   "http://localhost:8500",
			ServiceName:          "kyb-api",
			ServicePort:          8080,
			ServiceTags:          []string{"api", "kyb"},
			HealthCheckEndpoint:  "/health",
			Metadata: map[string]string{
				"version": "1.0.0",
				"env":     "production",
			},
		},
		HealthCheckConfig: &HealthCheckConfig{
			Enabled:                true,
			CheckInterval:          30 * time.Second,
			Timeout:                5 * time.Second,
			UnhealthyThreshold:     3,
			HealthyThreshold:       2,
			HealthCheckPath:        "/health",
			GracePeriod:            60 * time.Second,
			MaxConsecutiveFailures: 5,
		},
		AutoScalingConfig: &AutoScalingConfig{
			Enabled:               true,
			ScalingInterval:       60 * time.Second,
			CPUThreshold:          80.0,
			MemoryThreshold:       85.0,
			RequestRateThreshold:  1000.0,
			ResponseTimeThreshold: 500 * time.Millisecond,
			ScaleUpCooldown:       5 * time.Minute,
			ScaleDownCooldown:     10 * time.Minute,
			MaxScaleUpRate:        2,
			MaxScaleDownRate:      1,
			PredictiveScaling:     true,
			ScalingWindow:         10 * time.Minute,
		},
	}
}

// NewHorizontalScalingManager creates a new horizontal scaling manager
func NewHorizontalScalingManager(config *HorizontalScalingConfig) *HorizontalScalingManager {
	ctx, cancel := context.WithCancel(context.Background())

	manager := &HorizontalScalingManager{
		config:      config,
		ctx:         ctx,
		cancel:      cancel,
		scalingDone: make(chan struct{}),
	}

	// Initialize components
	if config.LoadBalancingEnabled {
		manager.loadBalancer = NewHorizontalScalingLoadBalancer(config.LoadBalancerConfig)
	}

	if config.ServiceDiscoveryEnabled {
		manager.serviceDiscovery = NewServiceDiscovery(config.ServiceDiscoveryConfig)
	}

	if config.HealthCheckEnabled {
		manager.healthChecker = NewHealthChecker(config.HealthCheckConfig)
	}

	if config.AutoScalingEnabled {
		manager.autoScaler = NewAutoScaler(config.AutoScalingConfig)
	}

	return manager
}

// Start starts the horizontal scaling manager
func (h *HorizontalScalingManager) Start() error {
	log.Println("ðŸš€ Starting horizontal scaling manager...")

	// Start service discovery
	if h.serviceDiscovery != nil {
		if err := h.serviceDiscovery.Start(); err != nil {
			return fmt.Errorf("failed to start service discovery: %w", err)
		}
	}

	// Start health checker
	if h.healthChecker != nil {
		if err := h.healthChecker.Start(); err != nil {
			return fmt.Errorf("failed to start health checker: %w", err)
		}
	}

	// Start auto scaler
	if h.autoScaler != nil {
		if err := h.autoScaler.Start(); err != nil {
			return fmt.Errorf("failed to start auto scaler: %w", err)
		}
	}

	// Start scaling loop
	go h.scalingLoop()

	log.Println("âœ… Horizontal scaling manager started successfully")
	return nil
}

// Stop stops the horizontal scaling manager
func (h *HorizontalScalingManager) Stop() error {
	log.Println("ðŸ›‘ Stopping horizontal scaling manager...")

	h.cancel()

	// Stop components
	if h.serviceDiscovery != nil {
		h.serviceDiscovery.Stop()
	}

	if h.healthChecker != nil {
		h.healthChecker.Stop()
	}

	if h.autoScaler != nil {
		h.autoScaler.Stop()
	}

	// Wait for scaling loop to finish
	select {
	case <-h.scalingDone:
	case <-time.After(10 * time.Second):
		log.Println("âš ï¸ Timeout waiting for scaling loop to stop")
	}

	log.Println("âœ… Horizontal scaling manager stopped")
	return nil
}

// scalingLoop runs the main scaling loop
func (h *HorizontalScalingManager) scalingLoop() {
	defer close(h.scalingDone)

	ticker := time.NewTicker(h.config.ScalingCooldown)
	defer ticker.Stop()

	for {
		select {
		case <-h.ctx.Done():
			return
		case <-ticker.C:
			h.performScaling()
		}
	}
}

// performScaling performs scaling decisions
func (h *HorizontalScalingManager) performScaling() {
	h.mu.Lock()
	defer h.mu.Unlock()

	// Get current metrics
	metrics, err := h.collectScalingMetrics()
	if err != nil {
		log.Printf("âŒ Failed to collect scaling metrics: %v", err)
		return
	}

	// Evaluate scaling decisions
	decisions := h.evaluateScalingDecisions(metrics)

	// Execute scaling decisions
	for _, decision := range decisions {
		if err := h.executeScalingDecision(decision); err != nil {
			log.Printf("âŒ Failed to execute scaling decision: %v", err)
		}
	}
}

// collectScalingMetrics collects metrics for scaling decisions
func (h *HorizontalScalingManager) collectScalingMetrics() (*ScalingMetrics, error) {
	metrics := &ScalingMetrics{
		LastUpdated: time.Now(),
	}

	// Collect CPU usage
	cpuPercent, err := cpu.Percent(0, false)
	if err == nil && len(cpuPercent) > 0 {
		metrics.CPUUsage = cpuPercent[0]
	}

	// Collect memory usage
	memInfo, err := mem.VirtualMemory()
	if err == nil {
		metrics.MemoryUsage = memInfo.UsedPercent
	}

	// Collect request rate and response time from load balancer
	if h.loadBalancer != nil {
		stats := h.loadBalancer.GetStats()
		metrics.RequestRate = float64(stats.TotalRequests) / time.Since(stats.LastUpdated).Seconds()
		metrics.ResponseTime = stats.AverageResponseTime
		metrics.ConnectionCount = stats.ActiveConnections
	}

	return metrics, nil
}

// evaluateScalingDecisions evaluates scaling decisions based on metrics
func (h *HorizontalScalingManager) evaluateScalingDecisions(metrics *ScalingMetrics) []ScalingDecision {
	var decisions []ScalingDecision

	config := h.config.AutoScalingConfig

	// Scale up conditions
	if metrics.CPUUsage > config.CPUThreshold ||
		metrics.MemoryUsage > config.MemoryThreshold ||
		metrics.RequestRate > config.RequestRateThreshold ||
		metrics.ResponseTime > config.ResponseTimeThreshold {

		decisions = append(decisions, ScalingDecision{
			Type:      ScalingDecisionTypeScaleUp,
			Reason:    "High resource usage detected",
			Instances: 1,
		})
	}

	// Scale down conditions
	if metrics.CPUUsage < config.CPUThreshold*0.5 &&
		metrics.MemoryUsage < config.MemoryThreshold*0.5 &&
		metrics.RequestRate < config.RequestRateThreshold*0.5 {

		decisions = append(decisions, ScalingDecision{
			Type:      ScalingDecisionTypeScaleDown,
			Reason:    "Low resource usage detected",
			Instances: 1,
		})
	}

	return decisions
}

// executeScalingDecision executes a scaling decision
func (h *HorizontalScalingManager) executeScalingDecision(decision ScalingDecision) error {
	log.Printf("ðŸ”„ Executing scaling decision: %s - %s", decision.Type, decision.Reason)

	switch decision.Type {
	case ScalingDecisionTypeScaleUp:
		return h.scaleUp(decision.Instances)
	case ScalingDecisionTypeScaleDown:
		return h.scaleDown(decision.Instances)
	default:
		return fmt.Errorf("unknown scaling decision type: %s", decision.Type)
	}
}

// scaleUp scales up the number of instances
func (h *HorizontalScalingManager) scaleUp(count int) error {
	log.Printf("ðŸ“ˆ Scaling up by %d instances", count)
	return nil
}

// scaleDown scales down the number of instances
func (h *HorizontalScalingManager) scaleDown(count int) error {
	log.Printf("ðŸ“‰ Scaling down by %d instances", count)
	return nil
}

// NewHorizontalScalingLoadBalancer creates a new load balancer
func NewHorizontalScalingLoadBalancer(config *LoadBalancerConfig) *HorizontalScalingLoadBalancer {
	lb := &HorizontalScalingLoadBalancer{
		config:    config,
		instances: make([]*ServiceInstance, 0),
		sessions:  make(map[string]*SessionInfo),
		stats:     &HorizontalScalingLoadBalancerStats{},
	}

	// Set load balancing strategy
	switch config.Strategy {
	case "round-robin":
		lb.strategy = &RoundRobinStrategy{}
	case "weighted":
		lb.strategy = &WeightedStrategy{}
	case "least-connections":
		lb.strategy = &LeastConnectionsStrategy{}
	case "ip-hash":
		lb.strategy = &IPHashStrategy{}
	case "random":
		lb.strategy = &RandomStrategy{}
	default:
		lb.strategy = &RoundRobinStrategy{}
	}

	return lb
}

// AddInstance adds a service instance to the load balancer
func (lb *HorizontalScalingLoadBalancer) AddInstance(instance *ServiceInstance) {
	lb.mu.Lock()
	defer lb.mu.Unlock()

	lb.instances = append(lb.instances, instance)
	log.Printf("âž• Added instance %s to load balancer", instance.ID)
}

// RemoveInstance removes a service instance from the load balancer
func (lb *HorizontalScalingLoadBalancer) RemoveInstance(instanceID string) {
	lb.mu.Lock()
	defer lb.mu.Unlock()

	for i, instance := range lb.instances {
		if instance.ID == instanceID {
			lb.instances = append(lb.instances[:i], lb.instances[i+1:]...)
			log.Printf("âž– Removed instance %s from load balancer", instanceID)
			return
		}
	}
}

// SelectInstance selects an instance using the configured strategy
func (lb *HorizontalScalingLoadBalancer) SelectInstance(request *http.Request) *ServiceInstance {
	lb.mu.RLock()
	defer lb.mu.RUnlock()

	if len(lb.instances) == 0 {
		return nil
	}

	// Check session affinity first
	if lb.config.SessionAffinity {
		if instance := lb.getSessionInstance(request); instance != nil {
			return instance
		}
	}

	// Filter healthy instances
	healthyInstances := make([]*ServiceInstance, 0)
	for _, instance := range lb.instances {
		if instance.HealthStatus == HealthStatusHealthy {
			healthyInstances = append(healthyInstances, instance)
		}
	}

	if len(healthyInstances) == 0 {
		return nil
	}

	// Select instance using strategy
	selected := lb.strategy.SelectInstance(healthyInstances, request)

	// Update session if session affinity is enabled
	if lb.config.SessionAffinity && selected != nil {
		lb.updateSession(request, selected.ID)
	}

	return selected
}

// getSessionInstance gets the instance for a session
func (lb *HorizontalScalingLoadBalancer) getSessionInstance(request *http.Request) *ServiceInstance {
	sessionID := lb.getSessionID(request)
	if sessionID == "" {
		return nil
	}

	lb.sessionMu.RLock()
	defer lb.sessionMu.RUnlock()

	session, exists := lb.sessions[sessionID]
	if !exists {
		return nil
	}

	// Check if session has expired
	if time.Since(session.LastAccess) > lb.config.SessionTimeout {
		delete(lb.sessions, sessionID)
		return nil
	}

	// Find the instance
	for _, instance := range lb.instances {
		if instance.ID == session.InstanceID && instance.HealthStatus == HealthStatusHealthy {
			return instance
		}
	}

	// Session instance is no longer available
	delete(lb.sessions, sessionID)
	return nil
}

// updateSession updates session information
func (lb *HorizontalScalingLoadBalancer) updateSession(request *http.Request, instanceID string) {
	sessionID := lb.getSessionID(request)
	if sessionID == "" {
		return
	}

	lb.sessionMu.Lock()
	defer lb.sessionMu.Unlock()

	lb.sessions[sessionID] = &SessionInfo{
		InstanceID:   instanceID,
		LastAccess:   time.Now(),
		RequestCount: 1,
	}
}

// getSessionID extracts session ID from request
func (lb *HorizontalScalingLoadBalancer) getSessionID(request *http.Request) string {
	// Try to get session ID from cookie
	if cookie, err := request.Cookie("session_id"); err == nil {
		return cookie.Value
	}

	// Try to get session ID from header
	if sessionID := request.Header.Get("X-Session-ID"); sessionID != "" {
		return sessionID
	}

	// Generate session ID based on client IP
	return request.RemoteAddr
}

// GetStats returns load balancer statistics
func (lb *HorizontalScalingLoadBalancer) GetStats() *HorizontalScalingLoadBalancerStats {
	return lb.stats
}

// UpdateStats updates load balancer statistics
func (lb *HorizontalScalingLoadBalancer) UpdateStats(responseTime time.Duration, success bool) {
	atomic.AddInt64(&lb.stats.TotalRequests, 1)

	if success {
		atomic.AddInt64(&lb.stats.SuccessfulRequests, 1)
	} else {
		atomic.AddInt64(&lb.stats.FailedRequests, 1)
	}

	// Update average response time (simplified calculation)
	lb.stats.AverageResponseTime = responseTime
	lb.stats.LastUpdated = time.Now()
}

// RoundRobinStrategy implementation
func (rr *RoundRobinStrategy) SelectInstance(instances []*ServiceInstance, request *http.Request) *ServiceInstance {
	if len(instances) == 0 {
		return nil
	}

	current := atomic.AddInt32(&rr.current, 1)
	index := int(current) % len(instances)
	return instances[index]
}

// WeightedStrategy implementation
func (ws *WeightedStrategy) SelectInstance(instances []*ServiceInstance, request *http.Request) *ServiceInstance {
	if len(instances) == 0 {
		return nil
	}

	// Calculate total weight
	totalWeight := 0
	for _, instance := range instances {
		totalWeight += instance.Weight
	}

	if totalWeight == 0 {
		// Fall back to round-robin
		current := atomic.AddInt32(&ws.current, 1)
		index := int(current) % len(instances)
		return instances[index]
	}

	// Select based on weight
	current := atomic.AddInt32(&ws.current, 1)
	weight := int(current) % totalWeight

	runningWeight := 0
	for _, instance := range instances {
		runningWeight += instance.Weight
		if weight < runningWeight {
			return instance
		}
	}

	return instances[0] // Fallback
}

// LeastConnectionsStrategy implementation
func (lc *LeastConnectionsStrategy) SelectInstance(instances []*ServiceInstance, request *http.Request) *ServiceInstance {
	if len(instances) == 0 {
		return nil
	}

	var selected *ServiceInstance
	minConnections := int32(math.MaxInt32)

	for _, instance := range instances {
		connections := atomic.LoadInt32(&instance.ConnectionCount)
		if connections < minConnections {
			minConnections = connections
			selected = instance
		}
	}

	return selected
}

// IPHashStrategy implementation
func (ih *IPHashStrategy) SelectInstance(instances []*ServiceInstance, request *http.Request) *ServiceInstance {
	if len(instances) == 0 {
		return nil
	}

	// Hash the client IP
	clientIP := request.RemoteAddr
	hash := 0
	for _, char := range clientIP {
		hash = (hash*31 + int(char)) % len(instances)
	}

	return instances[hash]
}

// RandomStrategy implementation
func (rs *RandomStrategy) SelectInstance(instances []*ServiceInstance, request *http.Request) *ServiceInstance {
	if len(instances) == 0 {
		return nil
	}

	// Generate random index
	bytes := make([]byte, 8)
	rand.Read(bytes)
	hash := 0
	for _, b := range bytes {
		hash = (hash*31 + int(b)) % len(instances)
	}

	return instances[hash]
}

// NewServiceDiscovery creates a new service discovery manager
func NewServiceDiscovery(config *ServiceDiscoveryConfig) *ServiceDiscovery {
	ctx, cancel := context.WithCancel(context.Background())

	return &ServiceDiscovery{
		config:    config,
		instances: make(map[string]*ServiceInstance),
		registry:  &InMemoryServiceRegistry{},
		ctx:       ctx,
		cancel:    cancel,
		done:      make(chan struct{}),
	}
}

// Start starts the service discovery
func (sd *ServiceDiscovery) Start() error {
	log.Println("ðŸ” Starting service discovery...")

	// Start registration loop
	if sd.config.RegistrationEnabled {
		go sd.registrationLoop()
	}

	// Start discovery loop
	go sd.discoveryLoop()

	log.Println("âœ… Service discovery started")
	return nil
}

// Stop stops the service discovery
func (sd *ServiceDiscovery) Stop() error {
	log.Println("ðŸ›‘ Stopping service discovery...")

	sd.cancel()

	select {
	case <-sd.done:
	case <-time.After(5 * time.Second):
		log.Println("âš ï¸ Timeout waiting for service discovery to stop")
	}

	log.Println("âœ… Service discovery stopped")
	return nil
}

// registrationLoop runs the service registration loop
func (sd *ServiceDiscovery) registrationLoop() {
	ticker := time.NewTicker(sd.config.RegistrationInterval)
	defer ticker.Stop()

	for {
		select {
		case <-sd.ctx.Done():
			return
		case <-ticker.C:
			sd.registerService()
		}
	}
}

// discoveryLoop runs the service discovery loop
func (sd *ServiceDiscovery) discoveryLoop() {
	defer close(sd.done)

	ticker := time.NewTicker(sd.config.DiscoveryInterval)
	defer ticker.Stop()

	for {
		select {
		case <-sd.ctx.Done():
			return
		case <-ticker.C:
			sd.discoverServices()
		}
	}
}

// registerService registers the current service
func (sd *ServiceDiscovery) registerService() {
	instance := &ServiceInstance{
		ID:           generateInstanceID(),
		URL:          fmt.Sprintf("http://localhost:%d", sd.config.ServicePort),
		Weight:       1,
		HealthStatus: HealthStatusHealthy,
		Metadata:     sd.config.Metadata,
	}

	if err := sd.registry.Register(sd.ctx, instance); err != nil {
		log.Printf("âŒ Failed to register service: %v", err)
	}
}

// discoverServices discovers available services
func (sd *ServiceDiscovery) discoverServices() {
	instances, err := sd.registry.GetInstances(sd.ctx, sd.config.ServiceName)
	if err != nil {
		log.Printf("âŒ Failed to discover services: %v", err)
		return
	}

	sd.mu.Lock()
	defer sd.mu.Unlock()

	// Update instances
	sd.instances = make(map[string]*ServiceInstance)
	for _, instance := range instances {
		sd.instances[instance.ID] = instance
	}

	log.Printf("ðŸ” Discovered %d service instances", len(instances))
}

// generateInstanceID generates a unique instance ID
func generateInstanceID() string {
	bytes := make([]byte, 8)
	rand.Read(bytes)
	return fmt.Sprintf("instance-%x", bytes)
}

// NewHealthChecker creates a new health checker
func NewHealthChecker(config *HealthCheckConfig) *HealthChecker {
	ctx, cancel := context.WithCancel(context.Background())

	return &HealthChecker{
		config:    config,
		instances: make(map[string]*HealthCheckResult),
		ctx:       ctx,
		cancel:    cancel,
		done:      make(chan struct{}),
	}
}

// Start starts the health checker
func (hc *HealthChecker) Start() error {
	log.Println("ðŸ¥ Starting health checker...")

	go hc.healthCheckLoop()

	log.Println("âœ… Health checker started")
	return nil
}

// Stop stops the health checker
func (hc *HealthChecker) Stop() error {
	log.Println("ðŸ›‘ Stopping health checker...")

	hc.cancel()

	select {
	case <-hc.done:
	case <-time.After(5 * time.Second):
		log.Println("âš ï¸ Timeout waiting for health checker to stop")
	}

	log.Println("âœ… Health checker stopped")
	return nil
}

// healthCheckLoop runs the health check loop
func (hc *HealthChecker) healthCheckLoop() {
	defer close(hc.done)

	ticker := time.NewTicker(hc.config.CheckInterval)
	defer ticker.Stop()

	for {
		select {
		case <-hc.ctx.Done():
			return
		case <-ticker.C:
			hc.performHealthChecks()
		}
	}
}

// performHealthChecks performs health checks on all instances
func (hc *HealthChecker) performHealthChecks() {
	log.Println("ðŸ¥ Performing health checks...")
}

// NewAutoScaler creates a new auto scaler
func NewAutoScaler(config *AutoScalingConfig) *AutoScaler {
	ctx, cancel := context.WithCancel(context.Background())

	return &AutoScaler{
		config:    config,
		instances: make(map[string]*ScalingMetrics),
		ctx:       ctx,
		cancel:    cancel,
		done:      make(chan struct{}),
	}
}

// Start starts the auto scaler
func (as *AutoScaler) Start() error {
	log.Println("ðŸ“Š Starting auto scaler...")

	go as.scalingLoop()

	log.Println("âœ… Auto scaler started")
	return nil
}

// Stop stops the auto scaler
func (as *AutoScaler) Stop() error {
	log.Println("ðŸ›‘ Stopping auto scaler...")

	as.cancel()

	select {
	case <-as.done:
	case <-time.After(5 * time.Second):
		log.Println("âš ï¸ Timeout waiting for auto scaler to stop")
	}

	log.Println("âœ… Auto scaler stopped")
	return nil
}

// scalingLoop runs the auto scaling loop
func (as *AutoScaler) scalingLoop() {
	defer close(as.done)

	ticker := time.NewTicker(as.config.ScalingInterval)
	defer ticker.Stop()

	for {
		select {
		case <-as.ctx.Done():
			return
		case <-ticker.C:
			as.evaluateScaling()
		}
	}
}

// evaluateScaling evaluates scaling decisions
func (as *AutoScaler) evaluateScaling() {
	log.Println("ðŸ“Š Evaluating scaling decisions...")
}
