package middleware

import (
	"bytes"
	"context"
	"encoding/json"
	"net/http"
	"net/http/httptest"
	"testing"
	"time"
)

func TestDefaultAlertingScalingConfig(t *testing.T) {
	config := DefaultAlertingScalingConfig()

	// Test basic configuration values
	if config.AlertingInterval != 30*time.Second {
		t.Errorf("Expected alerting interval 30s, got %v", config.AlertingInterval)
	}

	if config.ScalingInterval != 60*time.Second {
		t.Errorf("Expected scaling interval 60s, got %v", config.ScalingInterval)
	}

	if config.MetricRetentionPeriod != 24*time.Hour {
		t.Errorf("Expected metric retention period 24h, got %v", config.MetricRetentionPeriod)
	}

	// Test alert thresholds
	if config.AlertThresholds.CPUWarning != 70.0 {
		t.Errorf("Expected CPU warning threshold 70.0, got %f", config.AlertThresholds.CPUWarning)
	}

	if config.AlertThresholds.MemoryCritical != 85.0 {
		t.Errorf("Expected memory critical threshold 85.0, got %f", config.AlertThresholds.MemoryCritical)
	}

	// Test scaling policies
	if config.ScalingPolicies.MinInstances != 1 {
		t.Errorf("Expected min instances 1, got %d", config.ScalingPolicies.MinInstances)
	}

	if config.ScalingPolicies.MaxInstances != 10 {
		t.Errorf("Expected max instances 10, got %d", config.ScalingPolicies.MaxInstances)
	}

	// Test notification channels
	if len(config.NotificationChannels) == 0 {
		t.Error("Expected at least one notification channel")
	}

	// Test escalation policies
	if len(config.EscalationPolicies) == 0 {
		t.Error("Expected at least one escalation policy")
	}
}

func TestNewResourceAlertingScalingManager(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	manager := NewResourceAlertingScalingManager(config)

	if manager == nil {
		t.Fatal("Expected manager to be created")
	}

	if manager.config != config {
		t.Error("Expected config to be set")
	}

	if manager.alertEngine == nil {
		t.Error("Expected alert engine to be initialized")
	}

	if manager.scalingEngine == nil {
		t.Error("Expected scaling engine to be initialized")
	}

	if manager.metricCollector == nil {
		t.Error("Expected metric collector to be initialized")
	}

	if manager.escalationEngine == nil {
		t.Error("Expected escalation engine to be initialized")
	}

	if manager.notificationMgr == nil {
		t.Error("Expected notification manager to be initialized")
	}

	// Test shutdown
	err := manager.Shutdown()
	if err != nil {
		t.Errorf("Expected shutdown to succeed, got error: %v", err)
	}
}

func TestNewResourceAlertingScalingManagerWithNilConfig(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)

	if manager == nil {
		t.Fatal("Expected manager to be created with default config")
	}

	if manager.config == nil {
		t.Error("Expected default config to be used")
	}

	// Test shutdown
	err := manager.Shutdown()
	if err != nil {
		t.Errorf("Expected shutdown to succeed, got error: %v", err)
	}
}

func TestEnhancedMetricCollector_CollectMetrics(t *testing.T) {
	collector := NewEnhancedMetricCollector()

	metrics, err := collector.CollectMetrics()
	if err != nil {
		t.Fatalf("Expected metrics collection to succeed, got error: %v", err)
	}

	if metrics == nil {
		t.Fatal("Expected metrics to be returned")
	}

	// Test basic metrics
	if metrics.Timestamp.IsZero() {
		t.Error("Expected timestamp to be set")
	}

	if metrics.GoroutineCount <= 0 {
		t.Error("Expected goroutine count to be positive")
	}

	if len(metrics.CPUUsagePerCore) == 0 {
		t.Error("Expected CPU usage per core to be populated")
	}

	if metrics.CustomMetrics == nil {
		t.Error("Expected custom metrics map to be initialized")
	}

	// Test metric history
	if len(collector.metricHistory) == 0 {
		t.Error("Expected metric history to contain at least one entry")
	}
}

func TestAlertEngine_CheckAlerts(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	engine := NewAlertEngine(config)

	// Create test metrics that should trigger alerts
	metrics := &EnhancedMetrics{
		Timestamp:       time.Now(),
		CPUUsage:        95.0,                   // Above emergency threshold
		MemoryUsage:     88.0,                   // Above critical threshold
		GoroutineCount:  1500,                   // Above critical threshold
		ResponseTime:    600 * time.Millisecond, // Above warning threshold
		Throughput:      40.0,                   // Below critical threshold
		ErrorRate:       12.0,                   // Above critical threshold
		CPUUsagePerCore: make([]float64, 4),
		LoadAverage:     []float64{3.5, 3.2, 3.0}, // Above critical threshold
		CustomMetrics:   make(map[string]float64),
	}

	alerts := engine.CheckAlerts(metrics)

	if len(alerts) == 0 {
		t.Error("Expected alerts to be generated")
	}

	// Check for specific alert types
	alertTypes := make(map[AlertType]bool)
	alertLevels := make(map[AlertLevel]bool)

	for _, alert := range alerts {
		alertTypes[alert.Type] = true
		alertLevels[alert.Level] = true

		// Test alert structure
		if alert.ID == "" {
			t.Error("Expected alert ID to be set")
		}

		if alert.Timestamp.IsZero() {
			t.Error("Expected alert timestamp to be set")
		}

		if alert.CurrentValue == 0 && alert.ThresholdValue == 0 {
			t.Error("Expected alert values to be set")
		}

		if alert.Tags == nil {
			t.Error("Expected alert tags to be initialized")
		}

		if alert.Metadata == nil {
			t.Error("Expected alert metadata to be initialized")
		}
	}

	// Verify expected alert types
	expectedTypes := []AlertType{AlertTypeCPU, AlertTypeMemory, AlertTypeGoroutine, AlertTypePerformance}
	for _, expectedType := range expectedTypes {
		if !alertTypes[expectedType] {
			t.Errorf("Expected alert type %s to be present", expectedType)
		}
	}

	// Verify expected alert levels
	expectedLevels := []AlertLevel{AlertLevelWarning, AlertLevelCritical, AlertLevelEmergency}
	for _, expectedLevel := range expectedLevels {
		if !alertLevels[expectedLevel] {
			t.Errorf("Expected alert level %s to be present", expectedLevel)
		}
	}
}

func TestAlertEngine_AddAlert(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	engine := NewAlertEngine(config)

	alert := &EnhancedAlert{
		ID:             "test-alert-1",
		Type:           AlertTypeCPU,
		Level:          AlertLevelCritical,
		Title:          "Test Alert",
		Description:    "Test alert description",
		CurrentValue:   95.0,
		ThresholdValue: 85.0,
		Timestamp:      time.Now(),
		Tags:           make(map[string]string),
		Metadata:       make(map[string]interface{}),
	}

	engine.AddAlert(alert)

	// Check active alerts
	if len(engine.activeAlerts) != 1 {
		t.Errorf("Expected 1 active alert, got %d", len(engine.activeAlerts))
	}

	if engine.activeAlerts[alert.ID] != alert {
		t.Error("Expected alert to be in active alerts")
	}

	// Check alert history
	if len(engine.alertHistory) != 1 {
		t.Errorf("Expected 1 alert in history, got %d", len(engine.alertHistory))
	}

	if engine.alertHistory[0] != alert {
		t.Error("Expected alert to be in history")
	}
}

func TestAlertEngine_UpdateAdaptiveThresholds(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	config.AlertThresholds.AdaptiveEnabled = true
	engine := NewAlertEngine(config)

	metrics := &EnhancedMetrics{
		Timestamp:     time.Now(),
		CPUUsage:      60.0,
		MemoryUsage:   55.0,
		CustomMetrics: make(map[string]float64),
	}

	engine.UpdateAdaptiveThresholds(metrics)

	// Check that adaptive metrics were updated
	if engine.adaptiveMetrics.LastUpdated.IsZero() {
		t.Error("Expected adaptive metrics to be updated")
	}

	if engine.adaptiveMetrics.BaselineMetrics["cpu_usage"] == 0 {
		t.Error("Expected CPU baseline to be set")
	}

	if engine.adaptiveMetrics.MovingAverages["memory_usage"] == 0 {
		t.Error("Expected memory moving average to be set")
	}
}

func TestAutoScalingEngine_CheckScaling(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	engine := NewAutoScalingEngine(config)

	// Test scale up conditions
	scaleUpMetrics := &EnhancedMetrics{
		Timestamp:     time.Now(),
		CPUUsage:      85.0, // Above scale up threshold
		MemoryUsage:   50.0,
		CustomMetrics: make(map[string]float64),
	}

	scalingEvent := engine.CheckScaling(scaleUpMetrics)
	if scalingEvent == nil {
		t.Error("Expected scaling event for scale up conditions")
	} else {
		if scalingEvent.Type != ScalingTypeUp {
			t.Errorf("Expected scale up event, got %s", scalingEvent.Type)
		}

		if scalingEvent.InstancesAfter <= scalingEvent.InstancesBefore {
			t.Error("Expected instances to increase")
		}
	}

	// Update engine state as if scaling occurred
	engine.currentInstances = scalingEvent.InstancesAfter
	engine.lastScalingTime = time.Now()

	// Test scale down conditions (wait for cooldown)
	time.Sleep(100 * time.Millisecond)

	scaleDownMetrics := &EnhancedMetrics{
		Timestamp:     time.Now(),
		CPUUsage:      25.0, // Below scale down threshold
		MemoryUsage:   25.0, // Below scale down threshold
		CustomMetrics: make(map[string]float64),
	}

	// Should not scale due to cooldown
	scalingEvent = engine.CheckScaling(scaleDownMetrics)
	if scalingEvent != nil {
		t.Error("Expected no scaling event due to cooldown period")
	}

	// Reset cooldown for test
	engine.lastScalingTime = time.Now().Add(-10 * time.Minute)

	scalingEvent = engine.CheckScaling(scaleDownMetrics)
	if scalingEvent == nil {
		t.Error("Expected scaling event for scale down conditions")
	} else {
		if scalingEvent.Type != ScalingTypeDown {
			t.Errorf("Expected scale down event, got %s", scalingEvent.Type)
		}

		if scalingEvent.InstancesAfter >= scalingEvent.InstancesBefore {
			t.Error("Expected instances to decrease")
		}
	}
}

func TestAutoScalingEngine_PredictiveScaling(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	config.ScalingPolicies.PredictiveScalingEnabled = true
	engine := NewAutoScalingEngine(config)

	// Add some historical data
	metrics := []float64{50.0, 55.0, 60.0, 65.0, 70.0, 75.0}
	for _, value := range metrics {
		engine.updateMetricWindow("cpu_usage", value)
	}

	// Calculate trend
	trend := engine.calculateTrend("cpu_usage")
	if trend <= 0 {
		t.Error("Expected positive trend for increasing CPU usage")
	}

	// Test predictive scale up
	currentMetrics := &EnhancedMetrics{
		Timestamp:     time.Now(),
		CPUUsage:      75.0, // Current value with positive trend should trigger predictive scaling
		MemoryUsage:   50.0,
		CustomMetrics: make(map[string]float64),
	}

	shouldScale := engine.predictiveScaleUp(currentMetrics)
	if !shouldScale {
		t.Error("Expected predictive scaling to be triggered")
	}
}

func TestAutoScalingEngine_AddScalingEvent(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	engine := NewAutoScalingEngine(config)

	event := &ScalingEvent{
		ID:              "test-scaling-1",
		Type:            ScalingTypeUp,
		Trigger:         ScalingTriggerThreshold,
		Strategy:        ScalingStrategyConservative,
		InstancesBefore: 2,
		InstancesAfter:  3,
		Reason:          "CPU usage exceeded threshold",
		Timestamp:       time.Now(),
		Success:         true,
	}

	engine.AddScalingEvent(event)

	if len(engine.scalingHistory) != 1 {
		t.Errorf("Expected 1 scaling event in history, got %d", len(engine.scalingHistory))
	}

	if engine.scalingHistory[0] != event {
		t.Error("Expected scaling event to be in history")
	}
}

func TestEscalationEngine_CheckEscalation(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	engine := NewEscalationEngine(config)

	// Create an alert that matches the default escalation policy
	alert := &EnhancedAlert{
		ID:    "test-alert-escalation",
		Type:  AlertTypeCPU,
		Level: AlertLevelCritical,
		Title: "CPU Critical Alert",
	}

	engine.CheckEscalation(alert)

	// Check that escalation was started
	if len(engine.activeEscalations) == 0 {
		t.Error("Expected escalation to be started")
	}

	if len(engine.escalationHistory) == 0 {
		t.Error("Expected escalation event in history")
	}
}

func TestNotificationManager_SendNotifications(t *testing.T) {
	channels := []*NotificationChannel{
		{
			ID:       "test-channel-1",
			Type:     NotificationTypeWebhook,
			Endpoint: "/test/webhook",
			Enabled:  true,
			Filters:  []AlertLevel{AlertLevelCritical},
		},
	}

	manager := NewNotificationManager(channels)

	alert := &EnhancedAlert{
		ID:          "test-alert-notification",
		Type:        AlertTypeCPU,
		Level:       AlertLevelCritical,
		Title:       "Test Alert for Notifications",
		Description: "Test alert description",
		Timestamp:   time.Now(),
	}

	manager.SendNotifications(alert)

	// Check that notification was sent
	if len(manager.notificationHistory) == 0 {
		t.Error("Expected notification to be sent")
	}

	notification := manager.notificationHistory[0]
	if notification.AlertID != alert.ID {
		t.Error("Expected notification to reference the alert")
	}

	if notification.ChannelID != channels[0].ID {
		t.Error("Expected notification to use the correct channel")
	}
}

func TestResourceAlertingScalingManager_GetCurrentMetrics(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	metrics, err := manager.GetCurrentMetrics()
	if err != nil {
		t.Fatalf("Expected metrics to be retrieved, got error: %v", err)
	}

	if metrics == nil {
		t.Fatal("Expected metrics to be returned")
	}

	if metrics.Timestamp.IsZero() {
		t.Error("Expected timestamp to be set")
	}
}

func TestResourceAlertingScalingManager_ManualScale(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	config.ScalingPolicies.MinInstances = 1
	config.ScalingPolicies.MaxInstances = 5
	manager := NewResourceAlertingScalingManager(config)
	defer manager.Shutdown()

	// Test valid scaling
	err := manager.ManualScale(3, "Manual test scaling")
	if err != nil {
		t.Errorf("Expected manual scaling to succeed, got error: %v", err)
	}

	currentInstances := manager.GetCurrentInstances()
	if currentInstances != 3 {
		t.Errorf("Expected 3 instances, got %d", currentInstances)
	}

	// Test scaling below minimum
	err = manager.ManualScale(0, "Invalid scaling")
	if err == nil {
		t.Error("Expected error when scaling below minimum")
	}

	// Test scaling above maximum
	err = manager.ManualScale(10, "Invalid scaling")
	if err == nil {
		t.Error("Expected error when scaling above maximum")
	}

	// Check scaling history
	history := manager.GetScalingHistory(10)
	if len(history) == 0 {
		t.Error("Expected scaling history to contain events")
	}
}

func TestResourceAlertingScalingManager_AlertOperations(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	// Create and add an alert
	alert := &EnhancedAlert{
		ID:    "test-alert-ops",
		Type:  AlertTypeCPU,
		Level: AlertLevelWarning,
		Title: "Test Alert Operations",
	}

	manager.alertEngine.AddAlert(alert)

	// Test getting active alerts
	activeAlerts := manager.GetActiveAlerts()
	if len(activeAlerts) != 1 {
		t.Errorf("Expected 1 active alert, got %d", len(activeAlerts))
	}

	// Test acknowledging alert
	err := manager.AcknowledgeAlert(alert.ID)
	if err != nil {
		t.Errorf("Expected acknowledge to succeed, got error: %v", err)
	}

	if alert.AcknowledgedAt == nil {
		t.Error("Expected alert to be acknowledged")
	}

	// Test resolving alert
	err = manager.ResolveAlert(alert.ID)
	if err != nil {
		t.Errorf("Expected resolve to succeed, got error: %v", err)
	}

	if alert.ResolvedAt == nil {
		t.Error("Expected alert to be resolved")
	}

	// Test getting alert history
	history := manager.GetAlertHistory(10)
	if len(history) == 0 {
		t.Error("Expected alert history to contain alerts")
	}
}

func TestResourceAlertingScalingManager_UpdateConfig(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	newConfig := DefaultAlertingScalingConfig()
	newConfig.AlertingInterval = 60 * time.Second
	newConfig.AlertThresholds.CPUWarning = 75.0

	err := manager.UpdateConfig(newConfig)
	if err != nil {
		t.Errorf("Expected config update to succeed, got error: %v", err)
	}

	if manager.config.AlertingInterval != 60*time.Second {
		t.Error("Expected config to be updated")
	}

	if manager.alertEngine.thresholds.CPUWarning != 75.0 {
		t.Error("Expected thresholds to be updated")
	}
}

func TestResourceAlertingScalingManager_GetStatus(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	status := manager.GetStatus()

	expectedFields := []string{
		"status", "alerting_enabled", "scaling_enabled", "active_alerts",
		"current_instances", "min_instances", "max_instances",
		"predictive_enabled", "adaptive_enabled",
	}

	for _, field := range expectedFields {
		if _, exists := status[field]; !exists {
			t.Errorf("Expected status field %s to be present", field)
		}
	}
}

// API Tests

func TestResourceAlertingScalingAPI_GetActiveAlerts(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	api := NewResourceAlertingScalingAPI(manager)

	// Add test alert
	alert := &EnhancedAlert{
		ID:    "api-test-alert",
		Type:  AlertTypeMemory,
		Level: AlertLevelCritical,
		Title: "API Test Alert",
	}
	manager.alertEngine.AddAlert(alert)

	req, err := http.NewRequest("GET", "/v1/alerts/active", nil)
	if err != nil {
		t.Fatal(err)
	}

	rr := httptest.NewRecorder()
	api.GetActiveAlerts(rr, req)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}

	var response map[string]interface{}
	err = json.Unmarshal(rr.Body.Bytes(), &response)
	if err != nil {
		t.Fatalf("Failed to parse response: %v", err)
	}

	if alerts, exists := response["active_alerts"]; !exists {
		t.Error("Expected active_alerts field in response")
	} else if alertsSlice, ok := alerts.([]interface{}); !ok || len(alertsSlice) == 0 {
		t.Error("Expected at least one active alert")
	}
}

func TestResourceAlertingScalingAPI_GetScalingStatus(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	api := NewResourceAlertingScalingAPI(manager)

	req, err := http.NewRequest("GET", "/v1/scaling/status", nil)
	if err != nil {
		t.Fatal(err)
	}

	rr := httptest.NewRecorder()
	api.GetScalingStatus(rr, req)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}

	var response map[string]interface{}
	err = json.Unmarshal(rr.Body.Bytes(), &response)
	if err != nil {
		t.Fatalf("Failed to parse response: %v", err)
	}

	expectedFields := []string{
		"current_instances", "min_instances", "max_instances",
		"predictive_enabled", "cooldown_period",
	}

	for _, field := range expectedFields {
		if _, exists := response[field]; !exists {
			t.Errorf("Expected field %s in scaling status response", field)
		}
	}
}

func TestResourceAlertingScalingAPI_ManualScale(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	api := NewResourceAlertingScalingAPI(manager)

	scaleRequest := ManualScaleRequest{
		TargetInstances: 3,
		Reason:          "API test scaling",
	}

	requestBody, err := json.Marshal(scaleRequest)
	if err != nil {
		t.Fatal(err)
	}

	req, err := http.NewRequest("POST", "/v1/scaling/manual", bytes.NewBuffer(requestBody))
	if err != nil {
		t.Fatal(err)
	}
	req.Header.Set("Content-Type", "application/json")

	rr := httptest.NewRecorder()
	api.ManualScale(rr, req)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}

	var response map[string]interface{}
	err = json.Unmarshal(rr.Body.Bytes(), &response)
	if err != nil {
		t.Fatalf("Failed to parse response: %v", err)
	}

	if response["status"] != "success" {
		t.Error("Expected successful scaling response")
	}

	// Verify scaling occurred
	currentInstances := manager.GetCurrentInstances()
	if currentInstances != 3 {
		t.Errorf("Expected 3 instances after scaling, got %d", currentInstances)
	}
}

func TestResourceAlertingScalingAPI_AcknowledgeAlert(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	api := NewResourceAlertingScalingAPI(manager)

	// Add test alert
	alert := &EnhancedAlert{
		ID:    "api-ack-test-alert",
		Type:  AlertTypeCPU,
		Level: AlertLevelWarning,
		Title: "API Acknowledge Test Alert",
	}
	manager.alertEngine.AddAlert(alert)

	req, err := http.NewRequest("POST", "/v1/alerts/api-ack-test-alert/acknowledge", nil)
	if err != nil {
		t.Fatal(err)
	}

	// Mock PathValue function
	req = req.WithContext(context.WithValue(req.Context(), "alertId", "api-ack-test-alert"))

	rr := httptest.NewRecorder()

	// Create a mock request with PathValue
	mockReq := &mockRequestWithPath{
		Request: req,
		pathValues: map[string]string{
			"alertId": "api-ack-test-alert",
		},
	}

	api.AcknowledgeAlert(rr, mockReq)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}

	// Verify alert was acknowledged
	if alert.AcknowledgedAt == nil {
		t.Error("Expected alert to be acknowledged")
	}
}

func TestResourceAlertingScalingAPI_GetCurrentMetrics(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	api := NewResourceAlertingScalingAPI(manager)

	req, err := http.NewRequest("GET", "/v1/metrics/current", nil)
	if err != nil {
		t.Fatal(err)
	}

	rr := httptest.NewRecorder()
	api.GetCurrentMetrics(rr, req)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}

	var response map[string]interface{}
	err = json.Unmarshal(rr.Body.Bytes(), &response)
	if err != nil {
		t.Fatalf("Failed to parse response: %v", err)
	}

	if _, exists := response["metrics"]; !exists {
		t.Error("Expected metrics field in response")
	}
}

func TestResourceAlertingScalingAPI_GetHealth(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	api := NewResourceAlertingScalingAPI(manager)

	req, err := http.NewRequest("GET", "/v1/alerting-scaling/health", nil)
	if err != nil {
		t.Fatal(err)
	}

	rr := httptest.NewRecorder()
	api.GetHealth(rr, req)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}

	var response map[string]interface{}
	err = json.Unmarshal(rr.Body.Bytes(), &response)
	if err != nil {
		t.Fatalf("Failed to parse response: %v", err)
	}

	expectedFields := []string{
		"status", "alerting_operational", "scaling_operational",
		"metrics_operational", "notifications_operational",
	}

	for _, field := range expectedFields {
		if _, exists := response[field]; !exists {
			t.Errorf("Expected field %s in health response", field)
		}
	}
}

func TestResourceAlertingScalingAPI_RegisterRoutes(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	api := NewResourceAlertingScalingAPI(manager)
	mux := http.NewServeMux()

	// Should not panic
	api.RegisterResourceAlertingScalingRoutes(mux)

	// Test that routes are registered by making a request
	req, err := http.NewRequest("GET", "/v1/alerting-scaling/health", nil)
	if err != nil {
		t.Fatal(err)
	}

	rr := httptest.NewRecorder()
	mux.ServeHTTP(rr, req)

	// Should get a response (not 404)
	if status := rr.Code; status == http.StatusNotFound {
		t.Error("Expected route to be registered")
	}
}

// Mock request with PathValue support for testing
type mockRequestWithPath struct {
	*http.Request
	pathValues map[string]string
}

func (m *mockRequestWithPath) PathValue(key string) string {
	return m.pathValues[key]
}

func TestCreateNotificationChannel(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	api := NewResourceAlertingScalingAPI(manager)

	channel := NotificationChannel{
		Type:     NotificationTypeEmail,
		Endpoint: "test@example.com",
		Enabled:  true,
		Filters:  []AlertLevel{AlertLevelCritical},
	}

	requestBody, err := json.Marshal(channel)
	if err != nil {
		t.Fatal(err)
	}

	req, err := http.NewRequest("POST", "/v1/notifications/channels", bytes.NewBuffer(requestBody))
	if err != nil {
		t.Fatal(err)
	}
	req.Header.Set("Content-Type", "application/json")

	rr := httptest.NewRecorder()
	api.CreateNotificationChannel(rr, req)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}

	var response map[string]interface{}
	err = json.Unmarshal(rr.Body.Bytes(), &response)
	if err != nil {
		t.Fatalf("Failed to parse response: %v", err)
	}

	if response["status"] != "success" {
		t.Error("Expected successful channel creation")
	}

	// Verify channel was added
	if len(manager.config.NotificationChannels) != 2 { // 1 default + 1 new
		t.Errorf("Expected 2 notification channels, got %d", len(manager.config.NotificationChannels))
	}
}

func TestPredictiveModel(t *testing.T) {
	manager := NewResourceAlertingScalingManager(nil)
	defer manager.Shutdown()

	api := NewResourceAlertingScalingAPI(manager)

	// Test getting predictive model
	req, err := http.NewRequest("GET", "/v1/predictive/model", nil)
	if err != nil {
		t.Fatal(err)
	}

	rr := httptest.NewRecorder()
	api.GetPredictiveModel(rr, req)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}

	// Test training predictive model
	req, err = http.NewRequest("POST", "/v1/predictive/train", nil)
	if err != nil {
		t.Fatal(err)
	}

	rr = httptest.NewRecorder()
	api.TrainPredictiveModel(rr, req)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}

	// Test getting predictions
	req, err = http.NewRequest("GET", "/v1/predictive/predictions", nil)
	if err != nil {
		t.Fatal(err)
	}

	rr = httptest.NewRecorder()
	api.GetPredictions(rr, req)

	if status := rr.Code; status != http.StatusOK {
		t.Errorf("Expected status code %d, got %d", http.StatusOK, status)
	}
}

func TestNotificationRateLimiter(t *testing.T) {
	limiter := &NotificationRateLimiter{
		MaxNotifications: 3,
		TimeWindow:       time.Minute,
		notifications:    make([]time.Time, 0),
	}

	// Test that rate limiting works
	now := time.Now()

	// Add notifications within window
	for i := 0; i < 3; i++ {
		limiter.notifications = append(limiter.notifications, now)
	}

	// Should be at limit
	if len(limiter.notifications) != 3 {
		t.Errorf("Expected 3 notifications, got %d", len(limiter.notifications))
	}
}

func TestAlertSeverityMapping(t *testing.T) {
	config := DefaultAlertingScalingConfig()
	engine := NewAlertEngine(config)

	testCases := []struct {
		level    AlertLevel
		expected AlertSeverity
	}{
		{AlertLevelEmergency, AlertSeverityCritical},
		{AlertLevelCritical, AlertSeverityHigh},
		{AlertLevelWarning, AlertSeverityMedium},
		{AlertLevelInfo, AlertSeverityLow},
	}

	for _, tc := range testCases {
		severity := engine.levelToSeverity(tc.level)
		if severity != tc.expected {
			t.Errorf("Expected severity %d for level %s, got %d", tc.expected, tc.level, severity)
		}
	}
}

func TestScalingStrategies(t *testing.T) {
	strategies := []ScalingStrategy{
		ScalingStrategyConservative,
		ScalingStrategyAggressive,
		ScalingStrategyPredictive,
		ScalingStrategyAdaptive,
	}

	for _, strategy := range strategies {
		if string(strategy) == "" {
			t.Errorf("Strategy %s should have a non-empty string value", strategy)
		}
	}
}

func TestNotificationTypes(t *testing.T) {
	types := []NotificationType{
		NotificationTypeEmail,
		NotificationTypeSlack,
		NotificationTypeWebhook,
		NotificationTypeSMS,
		NotificationTypePagerDuty,
	}

	for _, notificationType := range types {
		if string(notificationType) == "" {
			t.Errorf("Notification type %s should have a non-empty string value", notificationType)
		}
	}
}
