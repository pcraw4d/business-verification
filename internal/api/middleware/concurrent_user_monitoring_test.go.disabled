package middleware

import (
	"testing"
	"time"
)

func TestDefaultMonitoringConfig(t *testing.T) {
	config := DefaultMonitoringConfig()

	if config.MonitoringInterval != 30*time.Second {
		t.Errorf("Expected 30 second monitoring interval, got %v", config.MonitoringInterval)
	}

	if config.OptimizationInterval != 5*time.Minute {
		t.Errorf("Expected 5 minute optimization interval, got %v", config.OptimizationInterval)
	}

	if config.MaxConcurrentUsers != 100 {
		t.Errorf("Expected 100 max concurrent users, got %d", config.MaxConcurrentUsers)
	}

	if config.PerformanceThresholds.MaxResponseTime != 5*time.Second {
		t.Errorf("Expected 5 second max response time, got %v", config.PerformanceThresholds.MaxResponseTime)
	}

	if config.PerformanceThresholds.MaxErrorRate != 0.05 {
		t.Errorf("Expected 0.05 max error rate, got %f", config.PerformanceThresholds.MaxErrorRate)
	}
}

func TestNewConcurrentUserMonitor(t *testing.T) {
	config := DefaultMonitoringConfig()
	sessionManager := NewSessionManager(nil)
	queue := NewRequestQueue(DefaultQueueConfig())

	monitor := NewConcurrentUserMonitor(config, sessionManager, queue)
	defer monitor.Shutdown()

	if monitor.config == nil {
		t.Error("Expected config to be set")
	}

	if monitor.sessionManager == nil {
		t.Error("Expected session manager to be set")
	}

	if monitor.queue == nil {
		t.Error("Expected queue to be set")
	}

	if monitor.metrics == nil {
		t.Error("Expected metrics to be initialized")
	}

	if monitor.optimizer == nil {
		t.Error("Expected optimizer to be initialized")
	}

	if monitor.alerts == nil {
		t.Error("Expected alerts to be initialized")
	}
}

func TestConcurrentUserMonitor_GetMetrics(t *testing.T) {
	config := DefaultMonitoringConfig()
	sessionManager := NewSessionManager(nil)
	queue := NewRequestQueue(DefaultQueueConfig())

	monitor := NewConcurrentUserMonitor(config, sessionManager, queue)
	defer monitor.Shutdown()

	// Get initial metrics
	metrics := monitor.GetMetrics()

	// Timestamp may be zero initially before first metrics collection
	// This is expected behavior

	// Metrics should be initialized to zero values
	if metrics.ActiveUsers != 0 {
		t.Errorf("Expected 0 active users initially, got %d", metrics.ActiveUsers)
	}

	if metrics.TotalRequests != 0 {
		t.Errorf("Expected 0 total requests initially, got %d", metrics.TotalRequests)
	}

	if metrics.PerformanceScore != 0 {
		t.Errorf("Expected 0 performance score initially, got %f", metrics.PerformanceScore)
	}
}

func TestConcurrentUserMonitor_CalculatePerformanceScores(t *testing.T) {
	config := DefaultMonitoringConfig()
	sessionManager := NewSessionManager(nil)
	queue := NewRequestQueue(DefaultQueueConfig())

	monitor := NewConcurrentUserMonitor(config, sessionManager, queue)
	defer monitor.Shutdown()

	// Create test metrics
	metrics := &ConcurrentUserMetrics{
		ActiveUsers:         50,
		TotalRequests:       1000,
		SuccessfulRequests:  950,
		FailedRequests:      50,
		AverageResponseTime: 2 * time.Second,
		CPUUsage:            60.0,
		MemoryUsage:         70.0,
		QueueSize:           25,
	}

	// Calculate scores
	monitor.calculatePerformanceScores(metrics)

	// Check user activity score
	expectedActivityScore := float64(50) / float64(100) * 100 // 50%
	if metrics.UserActivityScore != expectedActivityScore {
		t.Errorf("Expected user activity score %f, got %f", expectedActivityScore, metrics.UserActivityScore)
	}

	// Check performance score (should be reduced due to error rate and response time)
	if metrics.PerformanceScore <= 0 {
		t.Error("Expected positive performance score")
	}

	if metrics.PerformanceScore > 100 {
		t.Errorf("Expected performance score <= 100, got %f", metrics.PerformanceScore)
	}
}

func TestConcurrentUserMonitor_IdentifyBottlenecks(t *testing.T) {
	config := DefaultMonitoringConfig()
	sessionManager := NewSessionManager(nil)
	queue := NewRequestQueue(DefaultQueueConfig())

	monitor := NewConcurrentUserMonitor(config, sessionManager, queue)
	defer monitor.Shutdown()

	// Test error rate bottleneck
	metrics := &ConcurrentUserMetrics{
		ErrorRate: 0.10, // 10% error rate (above 5% threshold)
	}
	monitor.identifyBottlenecks(metrics)

	if metrics.BottleneckType != "Error Rate" {
		t.Errorf("Expected bottleneck type 'Error Rate', got %s", metrics.BottleneckType)
	}

	if metrics.BottleneckSeverity != "Critical" {
		t.Errorf("Expected bottleneck severity 'Critical', got %s", metrics.BottleneckSeverity)
	}

	// Test CPU usage bottleneck
	metrics = &ConcurrentUserMetrics{
		ErrorRate: 0.01, // Low error rate
		CPUUsage:  90.0, // High CPU usage (above 80% threshold)
	}
	monitor.identifyBottlenecks(metrics)

	if metrics.BottleneckType != "CPU Usage" {
		t.Errorf("Expected bottleneck type 'CPU Usage', got %s", metrics.BottleneckType)
	}

	if metrics.BottleneckSeverity != "High" {
		t.Errorf("Expected bottleneck severity 'High', got %s", metrics.BottleneckSeverity)
	}

	// Test no bottleneck
	metrics = &ConcurrentUserMetrics{
		ErrorRate:           0.01,            // Low error rate
		CPUUsage:            50.0,            // Low CPU usage
		MemoryUsage:         60.0,            // Low memory usage
		AverageResponseTime: 1 * time.Second, // Fast response time
		QueueSize:           10,              // Small queue
		RequestsPerSecond:   20.0,            // Good throughput
	}
	monitor.identifyBottlenecks(metrics)

	if metrics.BottleneckType != "None" {
		t.Errorf("Expected bottleneck type 'None', got %s", metrics.BottleneckType)
	}

	if metrics.BottleneckSeverity != "None" {
		t.Errorf("Expected bottleneck severity 'None', got %s", metrics.BottleneckSeverity)
	}
}

func TestConcurrentUserMonitor_CheckThresholdAlert(t *testing.T) {
	config := DefaultMonitoringConfig()
	sessionManager := NewSessionManager(nil)
	queue := NewRequestQueue(DefaultQueueConfig())

	monitor := NewConcurrentUserMonitor(config, sessionManager, queue)
	defer monitor.Shutdown()

	// Test alert creation
	monitor.checkThresholdAlert("Test Metric", 15.0, 10.0, "warning")

	// Get alerts
	alerts := monitor.GetAlertHistory()

	if len(alerts) == 0 {
		t.Error("Expected alert to be created")
		return
	}

	alert := alerts[0]
	if alert.Level != "warning" {
		t.Errorf("Expected alert level 'warning', got %s", alert.Level)
	}

	if alert.Metric != "Test Metric" {
		t.Errorf("Expected alert metric 'Test Metric', got %s", alert.Metric)
	}

	if alert.Value != 15.0 {
		t.Errorf("Expected alert value 15.0, got %f", alert.Value)
	}

	if alert.Threshold != 10.0 {
		t.Errorf("Expected alert threshold 10.0, got %f", alert.Threshold)
	}
}

func TestNewPerformanceOptimizer(t *testing.T) {
	config := DefaultMonitoringConfig()
	optimizer := NewPerformanceOptimizer(config)

	if optimizer.config == nil {
		t.Error("Expected config to be set")
	}

	if len(optimizer.strategies) == 0 {
		t.Error("Expected optimization strategies to be added")
	}

	// Check that strategies are added
	strategyNames := make(map[string]bool)
	for _, strategy := range optimizer.strategies {
		strategyNames[strategy.Name] = true
	}

	expectedStrategies := []string{
		"Queue Size Optimization",
		"Worker Pool Optimization",
		"Session Cleanup Optimization",
		"Rate Limiting Optimization",
	}

	for _, expected := range expectedStrategies {
		if !strategyNames[expected] {
			t.Errorf("Expected strategy '%s' to be present", expected)
		}
	}
}

func TestPerformanceOptimizer_Optimize(t *testing.T) {
	config := DefaultMonitoringConfig()
	optimizer := NewPerformanceOptimizer(config)

	// Test optimization with high queue size
	metrics := &ConcurrentUserMetrics{
		QueueSize: 150, // Above threshold of 100
	}

	event := optimizer.Optimize(metrics)

	if event == nil {
		t.Error("Expected optimization event to be created")
	}

	if event.Type != "queue_size_optimization" {
		t.Errorf("Expected optimization type 'queue_size_optimization', got %s", event.Type)
	}

	if !event.Success {
		t.Error("Expected optimization to be successful")
	}

	// Test optimization with high CPU usage
	metrics = &ConcurrentUserMetrics{
		QueueSize: 50,   // Below threshold
		CPUUsage:  90.0, // Above threshold of 80%
	}

	event = optimizer.Optimize(metrics)

	if event == nil {
		t.Error("Expected optimization event to be created")
	}

	if event.Type != "worker_pool_optimization" {
		t.Errorf("Expected optimization type 'worker_pool_optimization', got %s", event.Type)
	}

	// Test no optimization needed
	metrics = &ConcurrentUserMetrics{
		QueueSize:   50,   // Below threshold
		CPUUsage:    50.0, // Below threshold
		MemoryUsage: 60.0, // Below threshold
		ErrorRate:   0.01, // Below threshold
	}

	event = optimizer.Optimize(metrics)

	if event != nil {
		t.Error("Expected no optimization event when metrics are within thresholds")
	}
}

func TestNewAlertManager(t *testing.T) {
	config := DefaultMonitoringConfig()
	manager := NewAlertManager(config)

	if manager.config == nil {
		t.Error("Expected config to be set")
	}

	if len(manager.handlers) == 0 {
		t.Error("Expected alert handlers to be added")
	}

	// Check that handlers are added
	handlerNames := make(map[string]bool)
	for _, handler := range manager.handlers {
		handlerNames[handler.Name] = true
	}

	expectedHandlers := []string{
		"log_handler",
		"metrics_handler",
	}

	for _, expected := range expectedHandlers {
		if !handlerNames[expected] {
			t.Errorf("Expected handler '%s' to be present", expected)
		}
	}
}

func TestAlertManager_AddAlert(t *testing.T) {
	config := DefaultMonitoringConfig()
	manager := NewAlertManager(config)

	// Create test alert
	alert := AlertEvent{
		Timestamp: time.Now(),
		Level:     "warning",
		Type:      "test_alert",
		Message:   "Test alert message",
		Metric:    "test_metric",
		Value:     15.0,
		Threshold: 10.0,
		Resolved:  false,
	}

	// Add alert
	manager.AddAlert(alert)

	// Get alerts
	alerts := manager.GetAlerts()

	if len(alerts) != 1 {
		t.Errorf("Expected 1 alert, got %d", len(alerts))
	}

	retrievedAlert := alerts[0]
	if retrievedAlert.Level != alert.Level {
		t.Errorf("Expected alert level %s, got %s", alert.Level, retrievedAlert.Level)
	}

	if retrievedAlert.Type != alert.Type {
		t.Errorf("Expected alert type %s, got %s", alert.Type, retrievedAlert.Type)
	}

	if retrievedAlert.Value != alert.Value {
		t.Errorf("Expected alert value %f, got %f", alert.Value, retrievedAlert.Value)
	}
}

func TestAlertManager_ResolveAlert(t *testing.T) {
	config := DefaultMonitoringConfig()
	manager := NewAlertManager(config)

	// Create test alert
	alert := AlertEvent{
		Timestamp: time.Now(),
		Level:     "warning",
		Type:      "test_alert",
		Message:   "Test alert message",
		Metric:    "test_metric",
		Value:     15.0,
		Threshold: 10.0,
		Resolved:  false,
	}

	// Add alert
	manager.AddAlert(alert)

	// Resolve alert
	success := manager.ResolveAlert(0)

	if !success {
		t.Error("Expected alert resolution to succeed")
	}

	// Get alerts
	alerts := manager.GetAlerts()

	if len(alerts) != 1 {
		t.Errorf("Expected 1 alert, got %d", len(alerts))
	}

	resolvedAlert := alerts[0]
	if !resolvedAlert.Resolved {
		t.Error("Expected alert to be resolved")
	}

	if resolvedAlert.ResolvedAt.IsZero() {
		t.Error("Expected resolved timestamp to be set")
	}

	// Test resolving non-existent alert
	success = manager.ResolveAlert(999)
	if success {
		t.Error("Expected alert resolution to fail for non-existent alert")
	}
}

func TestConcurrentUserMonitor_GetOptimizationHistory(t *testing.T) {
	config := DefaultMonitoringConfig()
	sessionManager := NewSessionManager(nil)
	queue := NewRequestQueue(DefaultQueueConfig())

	monitor := NewConcurrentUserMonitor(config, sessionManager, queue)
	defer monitor.Shutdown()

	// Get initial history
	history := monitor.GetOptimizationHistory()

	if len(history) != 0 {
		t.Errorf("Expected empty optimization history initially, got %d entries", len(history))
	}
}

func TestConcurrentUserMonitor_GetAlertHistory(t *testing.T) {
	config := DefaultMonitoringConfig()
	sessionManager := NewSessionManager(nil)
	queue := NewRequestQueue(DefaultQueueConfig())

	monitor := NewConcurrentUserMonitor(config, sessionManager, queue)
	defer monitor.Shutdown()

	// Get initial history
	history := monitor.GetAlertHistory()

	if len(history) != 0 {
		t.Errorf("Expected empty alert history initially, got %d entries", len(history))
	}
}

func TestConcurrentUserMonitor_Shutdown(t *testing.T) {
	config := DefaultMonitoringConfig()
	sessionManager := NewSessionManager(nil)
	queue := NewRequestQueue(DefaultQueueConfig())

	monitor := NewConcurrentUserMonitor(config, sessionManager, queue)

	// Shutdown should complete without blocking
	done := make(chan bool)
	go func() {
		monitor.Shutdown()
		done <- true
	}()

	select {
	case <-done:
		// Shutdown completed successfully
	case <-time.After(5 * time.Second):
		t.Error("Shutdown timed out")
	}
}

func TestOptimizationEvent_Structure(t *testing.T) {
	event := OptimizationEvent{
		Timestamp:   time.Now(),
		Type:        "test_optimization",
		Description: "Test optimization description",
		Impact:      "medium",
		MetricsBefore: map[string]interface{}{
			"cpu_usage": 90.0,
		},
		MetricsAfter: map[string]interface{}{
			"cpu_usage": 70.0,
		},
		Success: true,
	}

	if event.Type != "test_optimization" {
		t.Errorf("Expected type 'test_optimization', got %s", event.Type)
	}

	if event.Description != "Test optimization description" {
		t.Errorf("Expected description 'Test optimization description', got %s", event.Description)
	}

	if event.Impact != "medium" {
		t.Errorf("Expected impact 'medium', got %s", event.Impact)
	}

	if !event.Success {
		t.Error("Expected success to be true")
	}

	if len(event.MetricsBefore) != 1 {
		t.Errorf("Expected 1 metric before, got %d", len(event.MetricsBefore))
	}

	if len(event.MetricsAfter) != 1 {
		t.Errorf("Expected 1 metric after, got %d", len(event.MetricsAfter))
	}
}

func TestAlertEvent_Structure(t *testing.T) {
	alert := AlertEvent{
		Timestamp: time.Now(),
		Level:     "warning",
		Type:      "threshold_exceeded",
		Message:   "Test alert message",
		Metric:    "test_metric",
		Value:     15.0,
		Threshold: 10.0,
		Resolved:  false,
	}

	if alert.Level != "warning" {
		t.Errorf("Expected level 'warning', got %s", alert.Level)
	}

	if alert.Type != "threshold_exceeded" {
		t.Errorf("Expected type 'threshold_exceeded', got %s", alert.Type)
	}

	if alert.Message != "Test alert message" {
		t.Errorf("Expected message 'Test alert message', got %s", alert.Message)
	}

	if alert.Metric != "test_metric" {
		t.Errorf("Expected metric 'test_metric', got %s", alert.Metric)
	}

	if alert.Value != 15.0 {
		t.Errorf("Expected value 15.0, got %f", alert.Value)
	}

	if alert.Threshold != 10.0 {
		t.Errorf("Expected threshold 10.0, got %f", alert.Threshold)
	}

	if alert.Resolved {
		t.Error("Expected resolved to be false")
	}
}
