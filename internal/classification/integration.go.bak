package classification

import (
	"context"
	"fmt"
	"io"
	"log"
	"net/http"
	"time"

	"github.com/pcraw4d/business-verification/internal/classification/repository"
	"github.com/pcraw4d/business-verification/internal/database"
)

// WebsiteScrapingResult represents the result of website content scraping
type WebsiteScrapingResult struct {
	Success       bool   `json:"success"`
	Content       string `json:"content"`
	ContentLength int    `json:"content_length"`
	Reason        string `json:"reason"`
	Error         string `json:"error,omitempty"`
}

// IntegrationService provides a simple interface for integrating classification services
type IntegrationService struct {
	container *ClassificationContainer
	logger    *log.Logger
}

// NewIntegrationService creates a new integration service
func NewIntegrationService(supabaseClient *database.SupabaseClient, logger *log.Logger) *IntegrationService {
	if logger == nil {
		logger = log.Default()
	}

	container := NewClassificationContainer(supabaseClient, logger)
	return &IntegrationService{
		container: container,
		logger:    logger,
	}
}

// ProcessBusinessClassification processes business classification using the new services
func (s *IntegrationService) ProcessBusinessClassification(
	ctx context.Context,
	businessName, description, websiteURL string,
) map[string]interface{} {
	s.logger.Printf("üîç Processing business classification for: %s", businessName)

	// Get services from container
	industryDetectionService := s.container.GetIndustryDetectionService()
	codeGenerator := s.container.GetCodeGenerator()

	// Perform industry detection
	var industryResult *IndustryDetectionResult
	var err error
	var scrapingStatus map[string]interface{}

	if websiteURL != "" {
		// Use website content analysis
		scrapingResult := s.scrapeWebsiteContent(websiteURL)
		industryResult, err = industryDetectionService.DetectIndustryFromContent(ctx, scrapingResult.Content)

		// Store scraping status for metadata
		scrapingStatus = map[string]interface{}{
			"success":        scrapingResult.Success,
			"reason":         scrapingResult.Reason,
			"content_length": scrapingResult.ContentLength,
		}
		if !scrapingResult.Success {
			scrapingStatus["error"] = scrapingResult.Error
		}
	} else {
		// Use business information analysis
		industryResult, err = industryDetectionService.DetectIndustryFromBusinessInfo(
			ctx,
			businessName,
			description,
			websiteURL,
		)
	}

	// Handle detection failure
	if err != nil {
		s.logger.Printf("‚ö†Ô∏è Industry detection failed: %v", err)
		industryResult = s.createDefaultResult("Detection failed, using default")
	}

	// Extract keywords for classification codes
	var keywords []string
	if industryResult != nil {
		keywords = industryResult.KeywordsMatched
	}

	// Generate classification codes
	var classificationCodes *ClassificationCodesInfo
	if industryResult != nil {
		classificationCodes, err = codeGenerator.GenerateClassificationCodes(
			ctx,
			keywords,
			industryResult.Industry.Name,
			industryResult.Confidence,
		)
		if err != nil {
			s.logger.Printf("‚ö†Ô∏è Classification code generation failed: %v", err)
			classificationCodes = nil
		}
	}

	// Validate classification codes
	if classificationCodes != nil {
		if err := codeGenerator.ValidateClassificationCodes(classificationCodes, industryResult.Industry.Name); err != nil {
			s.logger.Printf("‚ö†Ô∏è Classification code validation failed: %v", err)
		}
	}

	// Get code statistics (for future use)
	// var codeStats map[string]interface{}
	// if classificationCodes != nil {
	//     codeStats = codeGenerator.GetCodeStatistics(classificationCodes)
	// }

	// Generate a business ID for tracking
	businessID := fmt.Sprintf("biz_%d", time.Now().Unix())

	// Build response in the format expected by the frontend
	response := map[string]interface{}{
		"success":       true,
		"business_id":   businessID,
		"business_name": businessName,
		"description":   description,
		"website_url":   websiteURL,
		"classification": map[string]interface{}{
			"mcc_codes":   []map[string]interface{}{},
			"sic_codes":   []map[string]interface{}{},
			"naics_codes": []map[string]interface{}{},
		},
		"confidence_score": 0.5,
		"status":           "success",
		"timestamp":        time.Now().UTC().Format(time.RFC3339),
		"data_source":      "database_driven",
		"enhanced_features": map[string]string{
			"database_driven_classification": "active",
			"modular_architecture":           "active",
		},
	}

	// Add scraping status to metadata if website URL was provided
	if websiteURL != "" {
		response["website_scraping"] = scrapingStatus
	}

	// Add keywords from industry detection to the response
	if industryResult != nil && len(industryResult.KeywordsMatched) > 0 {
		response["keywords_matched"] = industryResult.KeywordsMatched
	}

	// Add classification codes if available
	if classificationCodes != nil {
		// Convert MCC codes
		if classificationCodes.MCC != nil {
			mccCodes := make([]map[string]interface{}, 0, len(classificationCodes.MCC))
			for _, code := range classificationCodes.MCC {
				mccCodes = append(mccCodes, map[string]interface{}{
					"code":        code.Code,
					"description": code.Description,
					"confidence":  code.Confidence,
				})
			}
			response["classification"].(map[string]interface{})["mcc_codes"] = mccCodes
		}

		// Convert SIC codes
		if classificationCodes.SIC != nil {
			sicCodes := make([]map[string]interface{}, 0, len(classificationCodes.SIC))
			for _, code := range classificationCodes.SIC {
				sicCodes = append(sicCodes, map[string]interface{}{
					"code":        code.Code,
					"description": code.Description,
					"confidence":  code.Confidence,
				})
			}
			response["classification"].(map[string]interface{})["sic_codes"] = sicCodes
		}

		// Convert NAICS codes
		if classificationCodes.NAICS != nil {
			naicsCodes := make([]map[string]interface{}, 0, len(classificationCodes.NAICS))
			for _, code := range classificationCodes.NAICS {
				naicsCodes = append(naicsCodes, map[string]interface{}{
					"code":        code.Code,
					"description": code.Description,
					"confidence":  code.Confidence,
				})
			}
			response["classification"].(map[string]interface{})["naics_codes"] = naicsCodes
		}

		// Update confidence score based on industry detection
		if industryResult != nil {
			response["confidence_score"] = industryResult.Confidence
		}
	}

	return response
}

// GetHealthStatus returns the health status of all classification services
func (s *IntegrationService) GetHealthStatus() map[string]interface{} {
	return s.container.HealthCheck()
}

// Close performs cleanup operations
func (s *IntegrationService) Close() error {
	return s.container.Close()
}

// createDefaultResult creates a default industry detection result
func (s *IntegrationService) createDefaultResult(reason string) *IndustryDetectionResult {
	return &IndustryDetectionResult{
		Industry: &repository.Industry{
			ID:   1,
			Name: "General Business",
		},
		Confidence:      0.5,
		KeywordsMatched: []string{},
		AnalysisMethod:  "Fallback (detection failed)",
		Evidence:        reason,
	}
}

// scrapeWebsiteContent performs basic website content scraping
func (s *IntegrationService) scrapeWebsiteContent(url string) *WebsiteScrapingResult {
	client := &http.Client{Timeout: 10 * time.Second}

	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		s.logger.Printf("‚ùå Error creating request for %s: %v", url, err)
		return &WebsiteScrapingResult{
			Success:       false,
			Content:       "",
			ContentLength: 0,
			Reason:        "Failed to create HTTP request",
			Error:         err.Error(),
		}
	}

	req.Header.Set("User-Agent", "Mozilla/5.0 (compatible; BusinessVerificationBot/1.0)")

	resp, err := client.Do(req)
	if err != nil {
		s.logger.Printf("‚ùå Error fetching content from %s: %v", url, err)
		return &WebsiteScrapingResult{
			Success:       false,
			Content:       "",
			ContentLength: 0,
			Reason:        "Failed to fetch website content",
			Error:         err.Error(),
		}
	}
	defer resp.Body.Close()

	// Check HTTP status code
	if resp.StatusCode != 200 {
		s.logger.Printf("‚ùå HTTP error %d for %s", resp.StatusCode, url)
		return &WebsiteScrapingResult{
			Success:       false,
			Content:       "",
			ContentLength: 0,
			Reason:        fmt.Sprintf("HTTP error %d", resp.StatusCode),
			Error:         fmt.Sprintf("Server returned status %d", resp.StatusCode),
		}
	}

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		s.logger.Printf("‚ùå Error reading response from %s: %v", url, err)
		return &WebsiteScrapingResult{
			Success:       false,
			Content:       "",
			ContentLength: 0,
			Reason:        "Failed to read response body",
			Error:         err.Error(),
		}
	}

	content := string(body)
	contentLength := len(content)

	// Check if content is too small (likely an error page)
	if contentLength < 100 {
		s.logger.Printf("‚ö†Ô∏è Very small content (%d chars) from %s, might be an error page", contentLength, url)
		return &WebsiteScrapingResult{
			Success:       false,
			Content:       content,
			ContentLength: contentLength,
			Reason:        "Content too small, likely an error page",
			Error:         "Content length less than 100 characters",
		}
	}

	s.logger.Printf("‚úÖ Successfully scraped %d characters from %s", contentLength, url)
	return &WebsiteScrapingResult{
		Success:       true,
		Content:       content,
		ContentLength: contentLength,
		Reason:        "Successfully scraped website content",
	}
}
